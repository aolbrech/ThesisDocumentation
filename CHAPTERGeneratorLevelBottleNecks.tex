
The goal is to obtain as fast as possible results on generator level in order to ensure that no bottlenecks are found when running MadWeight.
The advantage of only using generator level results is that one can be completely sure that the studied events are actual semi-leptonic ttbar events. Hence MadWeight should not have any problems calculating the weight for these kind of events and no CPU time will be spent on uncorrect events.\\
This implies that any deviation from the expected results implies a bias, or even a problem, concerning the MadWeight output.\\
\\
Once the results correspond to the expectations these preliminary results should be easily extended to reconstructed events. Finalizing the event selection then allows to fully trust the results obtained on reconstructed level and make sure that any deviation should be explained by the influence of the applied event selection.\\
These results can then be used to optimize the event selection with respect to the MadWeight output and CPU time needed.

\section{Transfer Functions}
In order to obtain results with MadWeight, the Transfer Functions which link the reconstructed energy distribution with the actual energy distributions should be calculated. In the case of generator level events this is not that important since no smearing of the energy is expected, but in order to avoid any bias from the used Transfer Functions it is adviced to use the real Transfer Functions with a smaller width.\\
\\
The method to obtain the parameters describing the energy smearing is (partly) explained in the PhD Thesis of Arnaud Pin\footnote{It should be noted that in the PhD Thesis of Arnaud, and in the current double Gaussian transfer function syntax in MadWeight, only 5 parameters are used. The narrow gaussian distribution doesn't have a normalisation parameter in front, but is normalized afterwards in the MadWeight code.} which can be found in:
\begin{eqnarray*}
 & & https://cp3.irmp.ucl.ac.be/upload/theses/phd/Pin\_Arnaud.pdf \\
& & /home/annik/Documents/Vub/PhD/ThesisSubjects/AnomalousCouplings/\\ & & PrepareGenLevelRunning\_Sep2014/TransferFunctions
\end{eqnarray*}

The method to obtain the parameters of the Transfer Functions is based on the code received from Petra and Lieselotte, which was used in the Master thesis of Lieselotte\footnote{Additional feedback was also received from Arnaud, but he was using a completely different unbinned likelihood method. Since this only arrived after a couple of weeks waiting, it was decided to continue with the binned likelihood method used by Petra and Lieselotte.}. However it should be noted that the original code only had $4$ bins due to limited statistics, and now $10$ bins are used.\\
This code is almost identical to the ROOT class FitSlicesY() but has some minor differences. Some of these have already been changed in order to match better with the ROOT class.\\
One of the most important differences between the two approaches was the treatment of the underflow and overflow bin. In the received code these two bins were respectively added to the first and last bin and hence included in the fitting range. This is not the desired behavior since the size of underflow/overflow bin can be relatively large compared to the first/last bin and significantly change the value of these bins. This would then imply that the position of the first and last bin is not located at the correct position and will, especially in the case of a limited number of bins, have a significant influence on the fit result.
Now these underflow and overflow bins are just discarded from the fit range and will have no influence on the final result.\\
\\
Another important, but useful, difference between the two methods is the number of histograms which are saved. The received code saves for each distribution which is considered the ProjectionY distribution together with the double Gaussian fit for this bin. This can not be changed in the ROOT class which only stores the distribution of the 6 parameters of the double Gaussian fit formula.\\
\\
However, even after carefully ensuring that both methods are identical the obtained results are not.
Up to now it is not clear what is the reason for the discrepancy between the two results and the only way to found out is comparing the distributions and results for a significant amount of statistics.\\
One possibility is the used fit ranges and number of bins. If the last bins are low on statistics their distribution might not agree with a double Gaussian distribution and hence result in a failed fit. Therefore the distribution for each ProjectionY bin is now closely studied for all of the considered histograms. This can be found is the following section \ref{subsec::FitRanges}.

\subsection{Creating the Transfer Function}
In this subsection the different steps which were performed in order to built the actual Transfer Functions and implemented in MadWeight will be discussed in detail. The Transfer Functions used in this analysis are assumed to be uncorrelated, as shortly discussed in the PhD thesis of Arnaud. This assumption is necessary since otherwise it would be impossible to build them ourselves. The implication of this assumption is given in Equation \ref{eq::TFAssumption} and should be checked by looking at $E$ vs $\theta$, $E$ vs $\phi$ and $\theta$ vs $\phi$ histograms.
\begin{equation} \label{eq::TFAssumption}
 W(E, \theta, \phi) = W(E) W(\theta) W(\phi)
\end{equation}

A first important remark which should be made is the choice for using $p_{T}$ dependent Transfer Functions, in stead of the general $E$-dependent Transfer Functions already implemented in MadWeight. The changes which have to be done in order to correcty implement the Transfer Function in MadWeight are discussed in \ref{subsubsec::PtDependency}

\subsubsection{Using $p_{T}$ dependency} \label{subsubsec::PtDependency}
The Transfer Function configuration files in MadWeight are flexible enough to change the kinematic variables used for the Transfer Function calculations. Hence it seems to be more relevant to utilize the transverse momentum in stead of the energy of the considered partons and jets. Especially since the LHCO file used in the MadWeight calculations has the transverse momentum as input variable. Therefore it seems much more useful and realistic to use this parameter and not the energy. This implies that the Transfer Function configuration file should be adapted to use the \textit{pt(p)} and \textit{pt(pexp)} variables and not the currently used \textit{p(0)} and \textit{pexp(0)}.\\
\\
After actually implementing the created Transfer Function files\footnote{This was tested the first time on 30 November 2014.} and trying to use this Transfer Function one issue showed up which is currently not completel solved yet. MadWeight contains hard-coded files which only allow the use of the E, THETA and PHI variable for the Transfer Function dependencies. Therefore the so-called ``block name'' PT is not accepted by the $change\_tf.py$ file. However it seems that there is no clear physical motivation for this limited choice of kinematic variables, but in order to be completely sure additional information is needed from Olivier and Pierre.\\
Once this issue is resolved the following file, containing the restricted list of kinematic variables, should be adapted\footnote{Another solution is to keep the block name equal to E but just use the transverse momentum information. However this might result in confusion when in some files the hard-coded E parameter gets written down (for example on plots).}:
\begin{eqnarray*}
 bin/internal/madweight/change\_tf.py
\end{eqnarray*}

\subsubsection{Creation of the Transfer Functions}
The Transfer Functions are created from the simulated $t\bar{t}$ sample which will be used throughout the entire analysis. A very small nTuple is created from this simulated sample containing only the TLorentzVector information of both generated and reconstructed particles. From this the necessary diagrams, such as the 2D distributions of $p_T$, $\theta$ and $\eta$ difference between the generated and reconstructed particle with respect to the generated value, are created and saved in a ROOT file:
\begin{eqnarray*}
 AnomalousCouplings/TFInformation/PlotsForTransferFunctions\_FromTree.root 
\end{eqnarray*}

This analyzer, called $TFFit.cc$, also performs the double Gaussian fit of these 2D histograms and afterwards the $E$-dependent calorimeter fit. The technicalities and specific details of these fit procedures are documented in the following class:
\begin{eqnarray*}
 AnomalousCouplings/PersonalClasses/src/TFCreation.cc \\
 AnomalousCouplings/PersonalClasses/interface/TFCreation.h
\end{eqnarray*}

The results of the two consecutive fits performed on the Y-projections of these 2D distributions are stored in another ROOT file, together with the original 2D histograms. Also the function form of the double Gaussian fit formula using the obtained fit parameters, added in order to test the robustness of the fit results outside the fitted range, can be found in this ROOT file.
\begin{eqnarray*}
 AnomalousCouplings/TFInformation/CreatedTFFromDistributions\_FromTree.root
\end{eqnarray*}

This analyzer also has the flexibility to perform the fit on the entire range or on pre-defined ranges set by the user. For this a separate function, called \textit{SetFitRange}, is created where for each histogram the fit range for each separate bin can be defined. This is extremely useful to optimize the doubleGaussian fit which has to cover both the peak and the tail of the distributions in order to correctly calculate the $6$ fit parameters.\\
Another useful aspect of this analyzer is the automatic creation of the necessary .dat Transfer Function files needed for implementation in MadWeight. This is done in the \textit{WriteTF} class and the created files are:
\begin{eqnarray*}
 AnomalousCouplings/TFInformation/TF\_user.dat \\
 AnomalousCouplings/TFInformation/transfer\_card\_user.dat
\end{eqnarray*}
This first file contains the functional form of both the $E$-dependent calorimeter fit and the functional form of how these $6$ parameters should be included in the double Gaussian formula. Also the width for the different kinematic variables is defined within this file. For the moment the method used in the Transfer Functions already implemented in MadWeight is followed, implying that the width of the Transfer Function is defined as the maximum of the $\sigma$-parameter of the two Gaussians considered in the double Gaussian fit. The only difference is that in stead of the generated kinematic information, which is the variable on the abscissa of the considered 2D histograms, the reconstructed one is used.\\
\\
The second file contains the actual values of the different fit parameters for all the considered kinematic variables and particle types. Therefore this file is an extensive list of values to which is referred in the previous $TF\_user.dat$ file. For each particle type and kinematic variable the numbering used should be unique such that the correct values are implemented in the functional forms of the used fit formulas.

\subsubsection{Splitting in separate $\vert \eta \vert$ bins}
Since the kinematic variables tend do depend on the pseudorapidity $\eta$ the considered 2D histograms are created for four distinctive $\vert \eta \vert$ regions. It has been chosen to split the barrel region into three separate bins while the entire endcap region is contained within one single bin. The chosen binning is given here:
\begin{eqnarray*}
 & \vert \eta \vert & \leq 0.375 \\
 0.375 < & \vert \eta \vert & \leq 0.750 \\
 0.750 < & \vert \eta \vert & \leq 1.450 \\
 1.450 < & \vert \eta \vert & \leq 2.50
\end{eqnarray*}

The analyzer mentioned above is developed in such a way that both the fit results for all events as the results for the four separate $\vert \eta \vert$ bins are stored together. Therefore all the results can always be compared in the created ROOT files and in the distinct $dat$ files. 

\subsubsection{Actual implemenation in MadWeight}
All the possible Transfer Functions which can be used in MadWeight can be found in the $Source/MadWeight/transfer\_function/data$ directory. Any file can be added to this list and used within MadWeight.\\
The relevant files used for the creation of the Transfer Functions are given below. The first one is the translation of the used $TF\_user.dat$ into a MadWeight readable file which can be implemented. The second file is only relevant around line $292$ where the function used for the Transfer Function creation is explained. This is the general MadWeight constructor file where all the different MadWeight functions are defined.
\begin{eqnarray} 
 Source/MadWeight/transfer\_function/transfer\_function.f \nonumber \\
 bin/internal/madweight\_interface.py \nonumber
\end{eqnarray}
The commands which have to be executed in order to build this MadWeight readable file for the Transfer Function are given here:
\begin{eqnarray} 
 ./bin/mw\_options \nonumber \\
 define\_transfer\_fct \nonumber
\end{eqnarray}

\subsection{Comparing ProjectionY distributions}\label{subsec::FitRanges}
Each of the considered histograms is a 2D histogram where the abscissa represents the transverse momentum of the generator level parton and the ordinate the difference between the generator level parton and the reconstructed matched particle. This is done for the difference in transverse momentum and in $\theta$ and $\phi$ angles.\\
All the interesting histograms can be created automatically, for each of the desired $\vert \eta \vert$ bins separately, using the following ROOT analyzer:
\begin{eqnarray*}
 AnomalousCouplings/TFInformation/FitDistributions/SaveFitHistograms.C
\end{eqnarray*}
This analyzer is able to create each of the histograms seperately, both as $pdf$ and $png$, and automatically saves all the histograms of one type in a large stacked canvas. This allows to quickly see the used 2D distributions for each of the particle types (b-jets, light jets, electrons and muons) and the three kinematic variables ($p_T$, $\eta$ and $\phi$). This stacked canvas is shown in Figure \ref{fig::ColorPlots}.  Also the general behavior of each of the fitted diagrams together with the overal $\chi^{2}$ distribution is created for each 2D histogram as can be seen from Figure \ref{fig::StackedHistoBJetPt}, showing this for the $p_T$ distribution of the b-jets. Finally the distribution of the $6$ double Gaussian fit parameters together with the fit result of the $E$-dependent calorimeter formula is also collected in a stacked canvas, as can be seen in Figure \ref{fig::FitParamsBJetPt}.\\
\\
\begin{figure}[!h]
  \centering
  \includegraphics[width = 0.9 \textwidth]{/home/annik/GitTopTree/TopBrussels/AnomalousCouplings/TFInformation/FitDistributions/FromTree/ColorPlots.png}
  \caption{Used 2D distributions for double Gaussian fit. \textbf{IMPROVE CAPTION!}} \label{fig::ColorPlots} 
\end{figure}
The 2D distributions for the difference in transverse momentum tend to show a slightly asymmetric behavior, as can be seen from Figure \ref{fig::ColorPlots}. This can be explained by the influence of the event selection, which has a difference effect on the generated particle than the reconstructed particle. This because a particle surviving the $p_T$ cut actually has a different $p_T$ value on generated level due to \textbf{bad resolution, detector effects (???)}. This effect is almost negligible for the $\phi$ and $\theta$ angles \textbf{(Definitely sure that this is the case ??)}.\\
\\
\begin{figure}[!h]
  \centering
  \includegraphics[width = 0.9 \textwidth]{/home/annik/GitTopTree/TopBrussels/AnomalousCouplings/TFInformation/FitDistributions/FromTree/BJet_DiffPtVsGenPt/Overview_FitDistributions.png}
  \caption{Distribution of the energy difference between the generator level parton and the corresponding light quark jet for each of the 10 considered bins and the overflow bin. All distributions were fitted with a double Gaussian function.} \label{fig::StackedHistoBJetPt}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width = 0.9 \textwidth]{/home/annik/GitTopTree/TopBrussels/AnomalousCouplings/TFInformation/FitDistributions/FromTree/BJet_DiffPtVsGenPt/Overview_FitParameters.png}
  \caption{Energy dependency of the 6 parameters of the double Gaussian fit function. The result of the double Gaussian fit for each $p_T$ bin is combined in a $p_T$ dependent histogram and then fitted with the Calorimeter energy function as explained in the PhD Thesis of Arnaud Pin.} \label{fig::FitParamsBJetPt}
\end{figure}

\newpage
\textbf{STILL TO REVIEW}
\newpage
From these fit results can be concluded that the double Gaussian distribution is well recovered for lower energy values. As can be seen from the very first histogram, the largest bulk of events have an energy value lower than 100 GeV. The low statistics in the higher energy range can partly explain the failing fitting performance. The double Gaussian function form is used in order to properly reconstruct the narrow peak together with the wider Gaussian distribution representing the tail of the energy distribution. However in the high parton energy range, this first narrow distribution dissapeared due to low statistics in this region. Hence fitting these distributions will indeed result in an unsuccesful fit status.\\
\\
A possible solution which can be considered is adding some of these histograms together and in this way enhancing the statistics in these combined bins. From the collection of distributions per bin could be concluded that the ``ok'' and ``failed'' fits are the ones with only about 2$\%$ of the total number of events. Hence it could be possible to adapt the code in such a way that in these cases the bin is combined with the following ones until a percentage higher than 3$\%$ is obtained.\\
Another solution consists of reducing the range of the fitted histograms and only fit the distributions with $E_{parton}$ $<$ 150GeV. However before continuing with this option it should be completely understood whether MadWeight is able to extrapolate in a correct way to higher energy values.
%**************************************************

\newpage
\section{Cross Section distribution for new grid}
%**************************************************

\section{First results: Wrong log(likelihood) minimum}

The first obtained MadWeight results for the enlarged grid ($V_L$ $\in$ $[0.8,1.2]$ and $V_R$ $\in$ $[-1,1]$) using only parton-level ttbar events did not result in the expected minimum of $(V_L,V_R)$ = $(1, 0)$. This can be seen from Figure \ref{fig::Likelihood}, which shows the distribution of the log(likelihood) for each point in the considered grid.\\ \\
\begin{figure}[!h]
 \centering
 \includegraphics[width = 0.9 \textwidth]{/home/annik/Documents/Vub/PhD/ThesisSubjects/AnomalousCouplings/UnderstandLikelihoodDistr_July2014/AnomCouplings_GenEvent_NoSelect_Oct3200Events_SingleGaussUsed/Likelihood_NoXSNorm.png}
 \caption{Distribution of the log(likelihood) for each point in the grid using 3200 parton-level positive semi-muonic ttbar events. The transfer function used to smear the parton-level kinematics is the single-gaussian function standard included in MadWeight.}
 \label{fig::Likelihood}
\end{figure}
One of the possible influences on the displaced minimim of the log(likelihood) distribution could be the normalisation of the Cross Section influence. This XS normalisation ($\frac{XS}{XS^{SM}}$) should be multiplied with the likelihood value, not the log(likelihood). Hence in order to correctly take this into account the obtained log(likelihood) value for each point in the grid should be corrected using the logarithm of this XS normalisation. The distribution of the normalisation on the Cross Section can be found in Figure \ref{fig::XSandLikelihoodNorm} together with the log(likelihood) distribution after correctly taking into account this XS normalisation.\\ \\
\begin{figure}[!h]
 \includegraphics[width = 0.45 \textwidth]{/home/annik/Documents/Vub/PhD/ThesisSubjects/AnomalousCouplings/UnderstandLikelihoodDistr_July2014/AnomCouplings_GenEvent_NoSelect_Oct3200Events_SingleGaussUsed/XSNorm.png}
 \includegraphics[width = 0.45 \textwidth]{/home/annik/Documents/Vub/PhD/ThesisSubjects/AnomalousCouplings/UnderstandLikelihoodDistr_July2014/AnomCouplings_GenEvent_NoSelect_Oct3200Events_SingleGaussUsed/Likelihood_XSNorm.png}
 \caption{Distribution of the XS normalisation for positive semi-muonic ttbar events (left) and distribution of the log(likelhood) after taking into account this normalisation. As in the previous figure 3200 positive semi-muonic have been used to obtain this distribution and a single gaussian transfer function has been applied to smear the kinematics of these parton-level events.}
 \label{fig::XSandLikelihoodNorm}
\end{figure}
The formula which has been used is the following (Equation \ref{eq::ProbMW}):
\begin{eqnarray}
 P(y \vert a) = \frac{1}{\textcolor{red}{\sigma(a)}*Acc(a)} \int W(y|x,a)*Eff(x,a) \vert M(x,a) \vert^{2} T(x,a) dx \label{eq::ProbMW}\\
 \mathcal{L} = \prod P(y \vert a)
\end{eqnarray}
The normalisation which has been applied in Figure \ref{fig::XSandLikelihoodNorm} is given in Equation \ref{eq::LikelihoodNorm}:
\begin{equation}\label{eq::LikelihoodNorm}
 \mathcal{L}_{Norm} = - ln(\sum P(y \vert a)*\frac{XS}{XS^{SM}}) = -ln(\mathcal{L}) - ln(\frac{XS}{XS^{SM}}*N)
\end{equation}
\textbf{It should be checked whether this is the correct method to take into account the normalisation of the XS. Currently it has been assumed that this XS normalisation should be applied for each weight and hence is multiplied with the number of considered events. In the case that this normalisation should just be multiplied with the overall likelihood value ($\mathcal{L}$) the sum over the number of considered events drops out of the equation implying a very small influence of the XS on the log(likelihood) distribution.}\\ \\

As highlighted in the general Matrix Element Method formula (Equation \ref{eq::ProbMW}), the probability to measure the observed quantities $y$ already has a normalisation factor for the cross section. This factor is defined as the channel cross section and is calculated using Equation \ref{eq::ChannelXSMW}:\\ \\
\begin{equation}\label{eq::ChannelXSMW}
 \sigma(a) = \int_{X_i} \vert M(x,a) \vert^{2} T(x,a) dx
\end{equation}
\textbf{Hence it should be investigated in detail whether this XS normalisation should still be applied. From the above equations could be concluded that the change in cross section is actually already incorporated in the weight obtained from MadWeight. This could make sense since MadWeight has all the necessary information to calculate the cross section for each point in the considered grid. The cross section values for each point have been calculated using MadGraph and the same model as used for the MadWeight calculations. Unfortunately it is not completely clear from the MadWeight documentation whether this is actually included in the weight or not.}

\section{Correct normalisation of Matrix Element probability}
As could be seen in Equation \ref{eq::ProbMW} a term $\sigma(a)$ is included in the general Matrix Element Techniques formula. However it is not clear whether this cross section normalisation is actually performed within the MadWeight calculations or whether this normalisation should be done afterwards.\\
This question is closely related to the order of the current obtained weight values with MadWeight. Up to now no weight larger than $10^{-22}$ have been obtained, resulting in a very large log(likelihood) value. \\
\\
This small value can be caused by many different reasons for which the most plausible ones are listed here:
\begin{itemize}
 \item The normalisation of the MadWeight probability should still be done and is not performed within the Matrix Element Techniques formula.\\ \textbf{This can only be ruled out by contacting the Madweight experts and asking explicitely what is done in the Madweight calculations. Also a possible hint could be found inside the MadWeight python files (but this should only be done if the received answer is not perfectly clear).}
 \item The smallness of the weight could be caused by an error inside the created FeynRules model. \textit{Should also look for the mail where one of the MadWeight experts (Olivier/Pierre or even Celine) answered about the possible explanation for the smallness of the weight and whether this implies some wrong assumptions).}\\ \textbf{A possible way to exclude that the origin of this problem is the AnomalousCouplings FeynRules model is by comparing the results for the top mass fit when both the SM FeynRules model and the AnomalousCouplings model is used. If the weights are also this small when the SM model is used this smallness should be solved by an additional normalisation factor.}
\end{itemize}

\paragraph{Update 31/10/2014: Probability function NOT normalized (according to mail Olivier)\\}
As was expected from the smallness of the obtained MadWeight probabilities should the cross section normalisation be applied afterwards. Only in the older versions of MadWeight (based on MG) was this normalisation included automatically.

\subsubsection{Measurement of top quark mass using Matrix Element Method}

\paragraph{Comparing SM model with AnomalousCouplings model\\}
As a first step the Feynmann diagrams belonging to the two different models should be compared. This information can be found in the \textit{index.html} file in the following directories (and the files should be opened using firefox on mtop since this is the only m-machine with a working browser):
\begin{eqnarray*}
 \tiny{/AnomalousCouplings/MadGraph5\_aMC@NLO/madgraph5/SM\_ttbarSemiMuPlus} \\
 \tiny{/AnomalousCouplings/MadGraph5\_aMC@NLO/madgraph5/ttbarSemiMuPlus\_QED2}
\end{eqnarray*}

\paragraph{Comparing SM cross section with MassiveLeptons cross section\\}
In order to be sure that both models have the same Standard Model base, the cross sections for both models have been compared. This resulted in an unexpected outcome, namely that the obtained cross sections differ significantly depending on which MadGraph version is used to generate the considered events. A summary can be found in Table \ref{table::MGXS}.
\begin{table}[h!]
 \centering
 \begin{tabular}{|c|c|c|c|c|c|}
  \hline
  \multirow{2}{*}{Top quark mass}	&  \multicolumn{2}{|c|}{MadGraph aMC@NLO}	& \multicolumn{2}{|c|}{MadGraph v155}  	\\
					&  SM model	& MassiveLeptons model		& SM model 	& MassiveLeptons model	\\
  \hline
    153 				& $9.23$ pb	& $9.645$ pb			& $6.692$ pb	& $6.984$ pb		\\
    163					& $11.12$ pb	& $11.63$ pb			& $7.844$ pb	& $8.199$ pb		\\
    173					& $12.98$ pb	& $13.54$ pb			& $8.897$ pb	& $9.281$ pb		\\
    183					& $14.77$ pb	& $15.4$ pb			& $9.884$ pb	& $10.3$ pb		\\
    193					& $16.5$ pb	& $17.22$ pb			& $10.78$ pb	& $11.25$ pb		\\
  \hline 
 \end{tabular} 
 \caption{Cross section values for semi-muonic (+) ttbar decay obtained using two different MadGraph versions.} \label{table::MGXS}
\end{table}

From this table can be seen that there is, for both considered MadGraph versions, a small difference between the SM FeynRules model and the MassiveLeptons one. This could be caused by the different treatment of the leptons. In the SM model they are considered to be massless while in the MassiveLeptons one they are defined to have their actual mass.\\
A larger differrence occurs when both MadGraph versions are compared. From the answer received by Olivier it is not clear whether this difference is worrysome or could be explained by the LO theoretical uncertainties. Should also be investigated whether this difference is related to the NLO behavior of the newest MadGraph version. In case the MadGraph v155 version is not up to NLO a difference in cross section is definitely expected.

%**************************************************