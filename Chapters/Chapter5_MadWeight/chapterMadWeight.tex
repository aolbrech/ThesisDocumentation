\chapter{Matrix Element method} \label{ch::MW}

The Matrix Element method is a powerful analysis technique widely used in experimental high energy physics.
It was originally developed after the discovery of the top quark with the purpose of maximally exploiting both the kinematic information available in a considered final state configuration and the theoretical knowledge of the matter and has now gained in significance due to the challenging conditions at the LHC.
\\
The main advantage of this method is that it calculates a propability for each event that represents whether the event kinematics correspond with the theoretical assumed model restrictions. These are then combined into an overall likelihood, which allows to identify the most optimal parameter value. (REWRITE)
\\
\\
All the necessary aspects of this Matrix Element method will be discussed in detail in this chapter, starting with the \textit{explanation} of the \textit{mathematical} framework of the method in Section~\ref{sec::MWTheory}.
Afterwards the necessary normalisations and calibrations (\textit{How can you call the TFs?)}) in order to apply this method on the collisions produced at the Large Hadron collider will be introduced in Section~\ref{sec::TF}.
Finally, the entire framework will be applied on a toy-model example in Section~\ref{sec::MEMExample}, where the mass of the top quark will be determined as an illustration.

\section{Theoretical framework} \label{sec::MWTheory}

The Matrix Element method is an appropriate analysis technique to apply in case the process of interest is a complex final state, typically containing several jets and missing energy.
Hence, for this reason it has been used extensively in top-quark physics and has even been applied more recently for the discovery of the Brout-Englert-Higgs boson. (\textbf{Refs!})
%the \textit{'aangewezen'} choice in case 
\\

As the name of the method suggests, the Matrix Element method extracts the desired theoretical information by explicitely calculating the leading-order matrix-element of the considered process.
Hence, in order to correctly apply this method on reconstructed events, which are influenced by ..., this matrix-element is multiplied with the parton distribution functions, discussed previously in Section~\ref{sec::PDF}. %(and the phase-space measure (d$\Phi$) -- relevant for this hadron-level part?).
Finally, in order to ensure the detector effects influencing the reconstructed events are transfered to the parton-level configuration a resolution function is introduced.
%The goal of the Matrix Element method is to extract theoretical information, in the form of a set of parameters $\alpha$, from a sample of experimental events (NOT OWN WORDS).
The full formula, defined as the event weight, takes the following form:

\begin{equation} \label{eq::MWEvtProb}
 P(x \vert \alpha) = \frac{1}{\sigma_{\alpha}} \int d\Phi(y) \, dq_{1} \, dq_{2} ~ f_{1}(q_{1}) \, f_{2}(q_{2}) \, \vert M_{\alpha}(y) \vert^{2} \, W(x,y)
\end{equation}
with $x$ the reconstructed event, $y$ the parton-level configuration, $\vert M_{\alpha}(y) \vert^{2}$ the squared matrix-element, d$\Phi$ the phase-space measure, $f_{i}(q_{i})$ the parton distribution functions and $W(x,y)$ the resolution or transfer function.
This event weight has to be normalised by the total cross-section, denoted as $\sigma_{\alpha}$, to ensure it corresponds to a probability density ($\int P(x \vert \alpha) = 1)$.
\\

These individual event probabilities are then combined into an overall Matrix Element method likelihood from which the optimal value of the considered variables $\alpha$ can be extracted.
The actual determination of the most probable value of $\alpha$ is performed through a likelihood maximisation method. However, in practice the event likelihoods are converted into negative likelihood values such that this optimal value is obtained by minimising $\NegLL$. 
%This extraction procedure is done by maximising the obtained likelihood $\mathcal{L}_{MEM}$, or more generally minimising the logarithmic likelihood values $\NegLL$.
\\
\begin{equation}
 \mathcal{L}_{MEM}(x \vert \alpha) = \prod_{i=1}^{n} P(x \vert \alpha)
\end{equation}

The Matrix Element method can be applied in two different manners, either by studying two competing hypotheses or otherwise by determining the most optimal parameter of the considered theoretical model.
In this analysis the second method will be applied in order to determine the anomalous couplings of the top-quark pair decay vertex, as will be explained in Chapter~\ref{ch::Analysis}. Hence several parameters $\alpha$, representing the right-handed tensor coupling $\gR$, will be scanned over such that the most optimal value can accurately be determined.
%Two general ways of applying this technique exist, either it compares two competing hypotheses or otherwise, as is the case in this analysis, it is used to determine the most optimal parameter of the introduced theoretical model.
\\

\textit{Need to mention somewhere that the event should be given completely and what is given as input! (Maybe then continue with the integration...}

\subsection{Evaluation of the event weights}

In principle, the Matrix Element method is supposed to provide the most powerful tool to extract theoretical information from a sample of experimental events.
In practice however, the challenging computation procedure is considered a serious limitation on the applicability of the method.
This because the Matrix Element method has to integrate over the convolution of the hard scattering process, the actual matrix-element, with the experimental information available on the final state, obtained from the transfer functions.
Such an integration technique is far from numerical efficient, such that a considerable amount of time is required to analyse a limited number of these experimental events.
\\
\\
Another serious difficulty the Matrix Element method is faced with is the fact that for each considered process a separate integration procedure has to be developed.
However, during the last decade significant effort has been \textit{put into place} in order to ensure this integration technique is efficiently adapted to the process that is calculated.
This resulted in the development of the MadWeight phase-space generator, which is based on the adaptive Monte Carlo integrator VEGAS~\cite{VEGAS}.
\\
The main advantage of this program is that it takes care of the integration in a fully automated manner.
In addition, the fact that this program is fully integrated in the MadGraph framework allows for an easy access to the matrix-element amplitudes and to the topology of the different Feynman diagrams of the considered process.
%As a solution, MadWeight~\cite{MadWeightPaper} has been developed, which takes care of this integration in a fully automated manner. In addition, since it is incorporated in hte MadGraph framework, it has access to the Feynman diagrams and thus the corresponding matrix-elements.
%This phase-space generator is based on the adaptive Monte Carlo integrator VEGAS~\cite{VEGAS} and creates an optimized phase-space mapping in order to efficiently integrate the product of the squared matrix-element and the transfer function.
\\
\textit{now something about this phase-space mapping... (?)}
\\

One important drawback of this complex integration procedure, besides the numerical inefficiency, is that for a tiny fraction of events it appears that the integration does not converge.
Hence when analysing the sample of experimental events after they have been calculated by MadWeight, the events for which an event weight equal to zero is recovered will be removed from the set of events.
This because only events should be analysed for which an likelihood value is calculated for each of the considered $\alpha$ parameters since otherwise it could introduce a bias.
However, this will not influence the overall result since for the sample which has been affected the most by this feature only about $0.7\%$ of the events had to be excluded. For the majority of considered samples none of the events had to be rejected.
\\

\textit{Important to mention about the fact that no uncertainty is available on the weights!}

\subsection{Details on model construction}

The fact that the matrix-element calculation is embedded in the MadGraph framework also facilitates the study of new physics models since they can be easily incorporated.
This feature has been exploited in this thesis where the theoretical description of the Wtb integration vertex included in the MadGraph Standard Model has been extended to include the anomalous coupling contributions.
\\
Hence, a model has been developed using the FeynRules~\cite{} software program ...


\subsection{Optimisations}
\textit{Are there more optimisations? Otherwise seperate section not really relevant ...}
\\

Since the Matrix Element method is very time-consuming, the calculation of the weights has to be performed in the most optimal manner and the number of permuations should be reduced if possible.
Hence, as was already mentioned in Section~\ref{subsec::JetCombi}, in this thesis only the permutations between the two light jets will be considered since the two jets originating from the b-quark decay have been assigned to a specific top-quark decay channel.

\subsection{Extraction example}
\textit{Maybe give an example of how the likelihood looks like? Or maybe keep this for the actual example...}

\section{Normalisations and calibrations (of hadron-level events)} \label{sec::TF}

\textit{Some general introduction relevant for both XS and TF ...}

Since the Matrix Element method uses the generator-level information of the hard scattering interaction, the energy of the partons in the event has to be smeared out in order to correspond with the situation existing for reconstructed events. Hence resolution functions has to be taken into account for the different types of considered partons, which specifies the kinematic region allowed during the integration procedure.
The details of these transfer functions are given in Section~\ref{subsec::TF}, which are described by either double Gaussian functions or delta-functions in this analysis.

\subsection{Resolution functions} \label{subsec::TF}

Besided the assumptions made in the developed theoretical model implemented in MadGraph, the most important simplificant of the Matrix Element method can be found in the resolution functions.
In order to determine this energy smearing of the generator-level events, has to be imposed that the transfer functions are factorised for the different kinematic variables.
Hence, it is assumed that the (transverse) energy $E$, the pseudo-rapidity $\eta$ and the \textit{azimuthal angle} $\phi$ are uncorrelated such that the transfer functions for these variables can be determined separately.
\\
\begin{equation}
 W(x,y) = \prod_{i=1}^{n} W_{i}(x^{i},y^{1}) = W_{i}^{E}(x^{i},y^i) \, W_{i}^{\eta}(x^i, y^i) \, W_{i}^{\phi}(x^i,y^i)
\end{equation}

Besides this factorised approach, the transfer function development is affected by several other assumptions in order to simplify the parametrisation of the different particles.
At first, the transfer functions for the pseudo-rapidity and azimuthal angle will be modelled by a Dirac delta function $\delta$ since they are believed to be well described.
Hence, the considered $\ttbar$ decay process still requies three separate resolution functions: one for the muons and the two remaining to describe the light-flavoured jets and the b-quark jets.
In this analysis, it is assumed that the transfer function of the muons can also be represented by a Dirac delta function since its energy is determined rather accurately by the CMS detector. (\textit{Maybe show a distribution to strengthen this statement!.})
\\
As a result, only the transfer functions for the two types of jets will have to be calculated. 
The most accurate parameterisation of describing the distribution of the energy difference $E_{parton} - E_{reco}$ is by a double Gaussian distribution.
This type of function allows for an accurate description of both the peak and the tail of the distribution, such that the energy smearing of the created transfer function will be correct for a wide energy regime.
Obtaining such an accurate description is rather important since it will allow to reduce the influence of the hadron-level corrections on the obtained outcome. Due to the introduced assumptions, the applicability of the Matrix Element method will need to be tested in detail in order to ensure it is not affected by a bias, as will be discussed in Section~\ref{sec::EstimatorProp}.


\subsection{Cross-section  normalisation}
\textit{Since it is here a rather general case, first the method for generator-level events can be mentioned and then the difference with reconstructed collision events can be made perfectly clear.}

An important aspect of the Matrix Element method is the normalisation of the event probability using the cross-section and acceptance, which might vary significantly for the considered configurations. For the top-quark mass example given in Section~\ref{sec::TopMass}, the effect of this normalisation was negligible but for the measurement of the anomalous coupling coefficient this factor has proven to be rather important. The method opted for in this analysis to determine these cross-section values will be explained in Section~\ref{subsec::XSReco}.

For the measurement of the right-handed tensor coupling the cross-section normalisation is a vital component.
Independent whether generator-level or reconstructed events are considered, without this normalisation applied the Matrix Element method does not result in the correct outcome.
The substantial influence of this normalisation component has been summarised in Figure~\ref{fig::XSInflGen}, which shows the overall $\chiSqMEM$ distribution prior to and after the cross-section normalisation has been taken into account. The considered sample has been created using the Standard Model configuration such that the minimum of the distribution should correspond to $0$.
%This because the observed variations of the overall event probability for the coupling coefficient are much smaller than was the case for the top-quark mass measurement such that the cross-section normalisation actually has a significant influence on the obtained outcome.
%***********************************
% --> Certain this is the reason?
% ==> Maybe good to think of an explanation why this cross-section normalisation is so much more important
%***********************************
\begin{figure}[h!t]
 \centering
 % Add here the gR gen-level result of FitDistributions_CalibCurve_SemiMu_RgR_AllDeltaTF_MGSampleSM_20000Evts_NoCuts_OuterBinsExclForFit_20000Evts.root with and without XS normalisation
 % Important: Cannot yet use the sample after the event-selection is applied because this still has to be explained!!
 % --> Do this without the fit maybe .. ?
 \includegraphics[width = 0.3 \textwidth]{image.png} \hspace{0.5cm}
 \includegraphics[width = 0.3 \textwidth]{image.png}
 \caption{Distribution of the overall $\chiSqMEM$-value obtained by analysing the right-handed tensor coupling using 20 000 generator-level events. The distribution on the left is without any normalisation applied while the right one corresponds to the normalised result.} \label{fig::XSInflGen}
\end{figure}

The significant impact of the cross-section normalisation on the outcome of the measurement implies that the cross-section values for the reconstructed-level analysis should be determined with great care.
However, in contrast to the easy access to generator-level samples with alternative coupling coefficients, generating similar samples containing reconstructed events is a rather challenging and time-consuming process.
As a result, it has been opted for in this thesis to derive the cross-section values for the reconstructed events from the generator-level ones.
This approach significantly facilitates the cross-section determination since any generated process by MadGraph automatically calculates the cross-section of the considered process.
%**********************
% Any other motivation why FastSim has not been considered?
% --> Certain that it would perfectly describe the SM samples??
%**********************
\\
\\
In order to ensure that the obtained generator-level cross-sections can easily be related to the reconstructed ones, the conditions present for the reconstructed collision events will be mimicked as closely as possible during the generation process. Hence the generator events have to fullfill the basic event selection requirements\footnote{Important to note here is that once these selection criteria are applied to the generated events, the obtained cross-section will actually be a combination of the cross-section of the underlying physics process and the acceptance of the considered event selection. Hence the term ``cross-section normalisation'' will \textbf{implicitely} imply the combined normalisation $\sigma \times A$ mentioned in Equation~\ref{eq::MWEvtProb}.} listed in Table~\ref{table::GenCuts}.
By applying a significant fraction of the full event selection chain onto the generated events, the expected relative difference in behaviour of each $\gR$ value on the considered kinematic constraints will be incorporated. As previously mentioned in Section~\ref{sec::CalibCurve}, the remaining event-selection criteria are supposed to be less sensitive to the value of the coupling coefficient.
%The remaining event selection criteria are believed to be less sensitive to the value of the coupling coefficient, thus their relative dependence will not be taken into account.
\\
\\
In addition, the generated processes are also selected in order to remain with a similar event signature as is the case in data. Hence the cross-section values have been determined using a combination of top-quark pair decay processes surrounded with additional jets. The actual number of considered processes has been limited to the $\ttbar$ decay with none, one and two additional jets since the contribution of the following decays quickly becomes negligible.
%********************************************
% --> Does this correspond to LO, NLO and NNLO or is this still something different??
% Question: Interesting to give some of the Feynman diagrams belonging to the different processes?
%********************************************
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.15 \textwidth]{image.png} \hspace{0.2cm}
 \includegraphics[width = 0.15 \textwidth]{image.png} \hspace{0.2cm}
 \includegraphics[width = 0.15 \textwidth]{image.png}
 \caption{Feynman diagrams for the different generator-level processes considered for the calculation of the cross-sections. \textbf{Relevant?}}
\end{figure}

Even with these two optimisations applied, this approach will not result in a perfect agreement with the selected events. For instance, it is simply not possible to include every aspect of the full event-selection chain in exactly the same way when generating the different processes.
Hence, the obtained cross-section values will be scaled in order to take into account the influence of these non-included event selection criteria.
For this an identical behaviour throughout the entire $\gR$ range is assumed such that each cross-section value will be multiplied with the factor $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$. 
%********************************************
% --> Think of any other non-included effects!!
%********************************************
\\
In this factor the term $\sigma_{SM}^{reco}$ represents the measured cross-section of the selected events while the cross-section obtained for the combined generator-level sample created using the Standard Model configuration is denoted by $\sigma_{SM}^{gen}$.
The cross-section of the selected events is determined by dividing the semi-leptonic $\ttbar$ event count obtained after the full event selection chain with the luminosity of this sample, which has been given previously in Table~\ref{table::Samples}. 
\\

The final result of the cross-section calculation can be found in Figure~\ref{fig::XSDistr}, which shows the distribution of the cross-section values obtained for the generator-level events using the approach discussed above. The cross-section values for the selected events are also given in this Figure, obtained by multiplying each of the former cross-sections with the fixed scaling factor $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$ = $0.134$.
\\
\textbf{Remark: Used luminosity and number of events do not seem to be correct!}
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.7 \textwidth]{Chapters/Chapter6_Analysis/Figures/XSDistributions.pdf}
 \caption{Overview of the distribution of the generator-level cross-sections for different $\gR$ values and the reconstructed ones derived from them by applying the ratio $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$.} \label{fig::XSDistr}
\end{figure}

%--------------------------------------------------------------------------------------------------------- \\
%Also here there is an additional complexity when considering reconstructed events, since the cross-section of the $\ttbar$ decay depends on the value of the coupling coefficients in the interaction vertex. For generator-level events, these values are accesible for each generated sample since MadGraph automatically determines the cross-section of each generated process.
%\\
%Hence the cross-sections for these reconstructed events are derived from the MadGraph predictions by carefully calculating the generator-level cross-sections in a regime comparable to data. 
%This condition has been achieved by combining the cross-sections for each $\gR$ coefficients when no, one and two additional jets are included in the event. 
%This will not result in a perfect match to data, but will bring the considered configuration a bit closer to reality. 
%\\
%Since the cross-section should be include the effect of the event selection, the different MadGraph samples have to fullfill the different kinematic requirements given in Table~\ref{table::GenCuts}. The three different contributions are then added in order to obtain an overall cross-section for the \textbf{inclusive} 2-jet case, for which the results have been summarised in the second column of Table~\ref{table::XSValues}.
%The third column contains the cross-section values that will be applied for the measurement using the reconstructed events, and have been obtained by scaling the cross-section for each $\gR$ value with the fraction $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$. This ratio corrects the generator-level cross-sections to the expected reco-level one and can be applied onto all $\gR$ configurations since the relative effect of the event selection is already been incorporated by applying the basic event selection requirements on the MadGraph samples. The value $\sigma_{SM}^{reco}$ has been determined by dividing the number of selected events with the total number of events present in the sample and multiplying this with the cross-section of the semi-leptonic $\ttbar$ sample, which thus corresponds to multiplying the selected number of events with the luminosity of the simulated sample. The distribution of the generator-level cross-sections and the reconstructed ones is given in Figure~\ref{fig::XSDistr} and serves as an easy way to determine the reconstructed cross-section for other $\gR$ values if required.
%
%\begin{table}[h!t]
%  \centering
%  \caption{Overview of the cross-section values used for the reconstructed events together with the inclusive $\ttbar$+2jet events from which they have been derived. \textbf{Maybe redundant if figure is %given ..}} \label{table::XSValues}
%  \small
%  %\begin{tabular}{|c||c|c|c|c||c|}
%  \begin{tabular}{|c|c|c|}
%   \hline
%%   $\gR$ coefficient 	& $\ttbar$+0j ($\pbinv$) 	& $\ttbar$+1j ($\pbinv$) 	& $\ttbar$+2j ($\pbinv$) 	& Incl. $\ttbar$+2j 	& $\ttbar$ reco ($\pbinv$) 	\\
%   $\gR$ coefficient 	& Incl. $\ttbar$ + 2 jets ($\pbinv$) 	& $\ttbar$ reco ($\pbinv$) 	\\
%   \hline
%   \hline
%   -0.2  		& 3.34036				& 0.947244 			\\
%   -0.15		& 4.01717				& 1.13624			\\
%   -0.1 		& 4.84056				& 1.36448			\\
%   -0.05		& 5.82329 				& 1.63952			\\
%   0.0 		& 6.98981 				& 1.96892			\\
%   0.05		& 8.37733 				& 2.36027			\\
%   0.1 		& 10.0153 				& 2.82111			\\
%   0.15		& 11.9024 				& 3.35903			\\
%   0.2 		& 14.0873 				& 3.98157			\\
%   \hline
%  \end{tabular}
%\end{table}

\section{Matrix Element method in practice} \label{sec::MEMExample}
\subsection{Extracting information from the likelihood}
Will use parabola through tree inner points! (No, because this does not work for the linearity curve!)\\
Thus will keep this as a systematic!

\subsection{Determining the top-quark mass using the Matrix Element method}
Sample created using 4000 correct semi-leptonic $\ttbar$ events.
