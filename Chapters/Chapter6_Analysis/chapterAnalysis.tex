\chapter{Measurement of anomalous couplings in top-quark decays} \label{ch::Analysis}

With all necessary objects and methods clearly established in the previous chapters, the focus can now finally shift towards the actual topic of this thesis (\textit{measurement}).
The goal of this measurement, for which all aspects will be rigorously discussed in this chapter, is to determine the right-handed tensor coupling of the Wtb interaction vertex using a Matrix Element method.
\\

However, before the measurement can be performed, additional research is still required to ensure the developed procedure to extract information from the Matrix Element method is behaving properly. This because such a technique is likely to be negatively influenced by the applied model approximations, a property that should be tested using a \textit{linearity test/bias determination} for which the details can be found in Section~\ref{sec::CalibCurve}.
Afterwards some specific characteristics of the analysis should be discussed, as will be done in Section~\ref{sec::RecoAdapt}.
Main focus will be on understanding the challenges associated with applying a Matrix Element method on reconstructed collision events.
Once the entire framework necessary to investigate these type of events is set up, the actual measurement will be discussed in Section~\ref{sec::Meas}. Also an overview of the relevant systematic uncertainties will be given here.

\section{Linearity test of the Matrix Element method} \label{sec::CalibCurve}
%--> Maybe even get a stronger title ...

Due to the complexity of the Matrix Element method, several model approximations have been applied, which all have the potential to influence the outcome in a significant manner. 
In addition, the developed procedure to extract the relevant information from the likelihood retrieved from the Matrix Element method is a viable candidate to introduce a bias (with respect to the coupling coefficient).
Hence in order to ensure neither the constructed model nor the developed minimisation procedure hamper the applicability of the Matrix Element method, a detailed \textit{linearity test} will be performed. 
\\
\\
This \textit{calibration procedure} starts by determining whether for the different coupling coefficients any deviation the expectation can be observed. For this, various samples created with alternative coupling coefficients will need to be created. In case the Matrix Element method behaves as should be, the minimum retrieved for each of these samples should correspond to the value used to create them. \textit{Otherwise the result is influenced by a bias such that afterwards the final result will have to be corrected for.}
%*************************
% Interesting to mention such a statement already here ??
% --> First stick to the generator-level result and the testing of the method, and only then discuss the reco results!
%*************************
\\

One of the more difficult aspects of this \textit{linearity test} is obtaining samples of reconstructed collision events generated with alternative coupling coefficients, this in contrast to the ease with which generator-level events can be created. As a result, it has been opted for in this thesis to perform this \textit{linearity test} with the latter type of events.
However, since the event-selection is one of the more likely candidates to expect a different effect depending on the considered coupling coefficient, the generated events will have to fullfill some basic selection requirements.
\\
These event-selection criteria, which have been summarised in Table~\ref{table::GenCuts}, are chosen to be in close agreement with the kinematic constraints applied in the full event-selection chain discussed in Chapter~\ref{ch::EvtSel}. Unfortunately the identification and additional fine-tuning criteria cannot be taken into account, but are on the other hand not expected to be as dependent on the value of the coupling coefficient.
\begin{table}[h!t]
 \centering
 \caption{Basic event selection applied to the generator-level events in order to partially mimic the situation existing for the reconstructed collision events.} \label{table::GenCuts}
 \begin{tabular}{c|c|c|c}
  Particle 	& $\pT$-cut 		& $\vert \eta \vert$-cut 	& $\Delta$R-cut 	\\
  \hline
  Jets 		& $>$ 30 $\GeV$ 	& $<$ 2.5			& $<$ 0.3		\\
  Muon		& $>$ 26 $\GeV$		& $<$ 2.1			& $<$ 0.3		\\
  Neutrino 	& $>$ 25 $\GeV$		& $<$ 2.5			& $<$ 0.3		
 \end{tabular}
\end{table}

The generator-level samples created for this linearity test using the MadGraph event-generation process contain \textcolor{red}{20 000} events, a value comparable in size to the number of selected data events. 
Since the considered right-handed tensor coupling, $\gR$, is supposed to be equal to $0$ in the Standard Model, the linearity test has been restricted to the range $\left[-0.15, 0.15\right]$.
This because the minimum-extraction procedure has proven to give rise to rather accurate/precise results, such that in case any bias would exist it has to be recovered in this limited range.
%Question: Mention that this decision was also driven by the fact that the fit is not accurate enough because there is not enough bin-information outside this range??
\\
\\
Each generated sample has been analysed by the Matrix Element method and its minimum has been determined following the exact same method as will be applied for the reconstructed collision events.
This result can then be compared with the expected value of the corresponding sample, which should correspond to the coefficient imposed during the generation process.
In order to have both a consistent model and method, the overall shape of the obtained results should be consistent with a straight line through the origin with slope equal to $1$.
This resemblance is indeed recovered, as can be seen in Figure~\ref{fig::CalibCurve}, for which the relevant parameters are given in Equation (\ref{eq::CalibCurveResults}).
\begin{equation} \label{eq::CalibCurveResults}
 \textrm{slope} = xxx \pm xxx \qquad \& \qquad \textrm{off-set} = xxx \pm xxx  
\end{equation}

\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/CalibrationCurve_SlopeComparision_DifferentCuts.pdf}
 \caption{Outcome of the linearity test for the created model, indicating the absence of a bias due to the assumed model approximations. The calibration is therefore described perfectly since it can be represented by a straight line with slope $1$.} \label{fig::CalibCurve}
\end{figure}
%********************************
%Question: Fit also done by excluding the outer bins?
%********************************

From the performed linearity test is clearly visible that the introduced model (and method) do not result in a bias.
However, in order to reinforce this statement/conclusion, a chi-sq test has been carried out.
\\
Since the performed linearity proves that the introduced model does not result in a bias and thus the measurements obtained with the discussed model can be trusted, no calibration will be applied to the obtained outcome. The small deviation from the expected curve is perfectly covered by the corresponding uncertainty (\textbf{Check if this is also the case for the intersect value}) and should thus not be taken into account.
\\

\textbf{Statistically compatible with a straight line --> do a chi-sq test!}

%This means that the linearity test to check whether the model approximations might have introduced a bias has to be performed using the generator-level results and afterwards the entire curve needs to be shifted to the correspond with the result obtained from the signal $\ttbar$ sample.

\section{Required MEM framework adaptations} \label{sec::RecoAdapt}
%**********************
% Can also just make the section about the event cleaning but give this XS calculation as an introduction!
%  --> Work with paragraphs maybe ? (not so interesting to use paragrapsh in section ...)
%**********************

%First of all, the cross-section for these type of events for all the considered coupling coefficients should be determined, which is not as straightforward as was the case for the generator-level events discussed in the previous chapter. The approach opted for in this thesis will be discussed in Section~\ref{sec::RecoAdapt}.
%The second characteristic to be discussed here focusses on the adaptations necessary to have the Matrix Element method correctly deal with reconstructed events. 
%The fact that these events are more prone to detector inefficiencies or misdetermined kinematics significantly complicates the calculations.

An important aspect of the Matrix Element method is the normalisation of the event probability using the cross-section and acceptance, which might vary significantly for the different configurations considered. For the top-quark mass example, the effect of this normalisation was negligible but for the measurement of the anomalous coupling coefficient this factor has proven to be rather important. 
Hence a dedicated calculation of these cross-section values has been done, as will be explained in Section~\ref{subsec::XSReco}.
%*********************************
% --> Check if this XS is actually important for the top-mass ..
%*********************************
\\

A second feature that requires some adaptations when considering reconstructed collision events is related with the possible detector inefficiencies, ill-defined kinematic variables and wrongly reconstructed event topologies by which these type of events might be affected.
%Analysing reconstructed collision events using a Matrix Element method deviates significantly from any generator-level study, mainly because the former type of events tend to be influenced by detector inefficiencies, ill-defined kinematic variables or wrongly reconstructed event topologies.
Since the Matrix Element method will treat all events as if they were real semi-leptonic top-quark pair decays, any inconsistency from the expected topology is likely to result in the underlying mathematical framework to misdetermine the event probability.
Therefore it is of crucial importance a procedure is developed in order to exclude the contribution of these sort of events, for which the details will be given in Section~\ref{subsec::EvtCleaning}.

%In order to correctly apply the Matrix Element method on reconstructed collision events, a different approach than the one used for generator-level events is required.
%At first, the ease \textbf{with which} generator-level samples with an alternative mass or coupling (\textit{Check whether any example of couplings is given ...}) could be created can not be repeated for these reconstructed events. Therefore, the linearity test performed to test the validity of the considered model assumptions will be performed using generator-level events, as explained in Section~\ref{subsec::CalibCurve}. Also the cross-section values at non-SM coupling coefficients for the reconstructed events need to be extrapolated from these generator-level events, a procedure which will be discussed in Section~\ref{subsec::XSReco}.
%\\
%Secondly, and more related to the actual application of the technique, the possible influence of detector inefficiencies, ill-defined kinematic variables or wrongly reconstructed event topologies has a detrimental effect on the calculation of the event probability. 
%Therefore an additional event cleaning procedure, outlined in detail in Section~\ref{subsec::EvtCleaning}, is required in order to limit the importance of these type of events. 

\subsection{Cross-section calculation} \label{subsec::XSReco}

The cross-section normalisation is a vital component for the measurement of the anomalous couplings, both on the generator as reconstructed level. Without this correction factor applied, the Matrix Element method minimisation procedure does not allow to retrieve the correct outcome. The considerable effect of this normalisation component has been summarised in Figure~\ref{fig::XSInflGen}, which shows the overall $\chiSqMEM$ distribution prior to and after the cross-section normalisation has been taken into account for a sample of 20 000 generator-level events produced with the Standard Model configuration.
%This because the observed variations of the overall event probability for the coupling coefficient are much smaller than was the case for the top-quark mass measurement such that the cross-section normalisation actually has a significant influence on the obtained outcome.
%***********************************
% --> Certain this is the reason?
% ==> Maybe good to think of an explanation why this cross-section normalisation is so much more important
%***********************************
\begin{figure}[h!t]
 \centering
 % Add here the gR gen-level result of FitDistributions_CalibCurve_SemiMu_RgR_AllDeltaTF_MGSampleSM_20000Evts_NoCuts_OuterBinsExclForFit_20000Evts.root with and without XS normalisation
 % Important: Cannot yet use the sample after the event-selection is applied because this still has to be explained!!
 % --> Do this without the fit maybe .. ?
 \includegraphics[width = 0.3 \textwidth]{image.png} \hspace{0.5cm}
 \includegraphics[width = 0.3 \textwidth]{image.png}
 \caption{Distribution of the overall $\chiSqMEM$-value obtained by analysing the right-handed tensor coupling using 20 000 generator-level events. The distribution on the left is without any normalisation applied while the right one corresponds to the normalised result.} \label{fig::XSInflGen}
\end{figure}

The substantial impact of the cross-section normalisation on the outcome of the measurement implies that the cross-section values for the reconstructed-level analysis should be determined with great care.
However, in contrast to the easy access to generator-level samples with alternative coupling coefficients, generating similar samples containing reconstructed events is a rather challenging and time-consuming process.
As a result, it has been opted for in this thesis to derive the cross-section values for the reconstructed events from the generator-level ones.
%\\
%However, it is important to note that in the remainder of this analysis the normalisation of the Matrix Element probability by the cross-section and the acceptance values will be combined into one single normalisation. This because the cross-section will be altered by the acceptance such that the latter can be taken into account in a rather straightforward manner.
%**********************
% Any other motivation why FastSim has not been considered?
% --> Certain that it would perfectly describe the SM samples??
%**********************
\\

The chosen approach of deriving the cross-sections from generator-level events significantly facilitates the cross-section determination since any generated process by MadGraph automatically calculates the cross-section of the considered process.
In order to ensure that the obtained generator-level cross-sections can easily be related to the reconstructed ones, the conditions present for the reconstructed collision events will be mimicked as closely as possible during the generation process. Hence the generator events have to fullfill a couple of rather basic event selection requirements\footnote{Important to note here is that once these selection criteria are applied to the generated events, the obtained cross-section will actually be a combination of the cross-section of the underlying physics process and the acceptance of the considered event selection. Hence the term ``cross-section normalisation'' will \textbf{implicitely} imply the combined normalisation $\sigma \times A$ mentioned in Equation~\ref{eq::MWEvtProb}.}, which have been listed in Table~\ref{table::GenCuts}.

\begin{table}[h!t]
 \centering
 \caption{Basic event selection applied to the generator-level events in order to partially mimic the situation existing for the reconstructed collision events.} \label{table::GenCuts}
 \begin{tabular}{c|c|c|c}
  Particle 	& $\pT$-cut 		& $\vert \eta \vert$-cut 	& $\Delta$R-cut 	\\
  \hline
  Jets 		& $>$ 30 $\GeV$ 	& $<$ 2.5			& $<$ 0.3		\\
  Muon		& $>$ 26 $\GeV$		& $<$ 2.1			& $<$ 0.3		\\
  Neutrino 	& $>$ 25 $\GeV$		& $<$ 2.5			& $<$ 0.3		
 \end{tabular}
\end{table}

By applying a significant fraction of the full event selection chain onto the generated events, the expected relative difference in behaviour of each $\gR$ value on the considered kinematic constraints will be incorporated. The remaining event selection criteria are believed to be less sensitive to the value of the coupling coefficient, thus their relative dependence will not be taken into account.
\\

The considered generator process was not merely the general semi-leptonic top-quark pair decay but, again motivated by the fact that the generator configuration should resemble as closely as possible the reconstructed one, a combination of $\ttbar$ decays surrounded by additonal jets.
The different decays for which generator-level samples have been created are the semi-leptonic $\ttbar$ decays with none, one and two additional jets present in the event.
%********************************************
% --> Does this correspond to LO, NLO and NNLO or is this still something different??
% Question: Interesting to give some of the Feynman diagrams belonging to the different processes?
%********************************************
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.15 \textwidth]{image.png} \hspace{0.2cm}
 \includegraphics[width = 0.15 \textwidth]{image.png} \hspace{0.2cm}
 \includegraphics[width = 0.15 \textwidth]{image.png}
 \caption{Feynman diagrams for the different generator-level processes considered for the calculation of the cross-sections.}
\end{figure}


This approach is not able to exactly describe the situation as it occurs for the selected events since, for example, it is not possible to include the effect of the applied b-tagging requirement \textit{in exactly the same way as in data}. Hence, the obtained cross-section values will need to be scaled in order to take into account the influence of the b-jet identification algorithm and other non-included event selection criteria such as the isolation, the invariant mass constraints, $\dots$ (\textbf{etc.}?).
For this scaling a flat behaviour throughout the entire $\gR$ range is assumed such that each cross-section value is multiplied with the factor $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$. 
%********************************************
% --> Think of any other non-included effects!!
%********************************************
\\
The cross-section for the Standard Model configuration of the selected events, denoted as $\sigma_{SM}^{reco}$, is determined by dividing the semi-leptonic $\ttbar$ event count obtained after the full event selection chain has been applied \textbf{with} the luminosity of this simulated sample, which can be found in Table~\ref{table::Samples}. The reconstructed cross-section values, together with the generator-level ones from which they have been derived, is given in Figure~\ref{fig::XSDistr}.
%Even though this will not result in a perfect match to the different decays present in the selected events, it will definitely ensure the considered configuration corresponds more to reality.
\\
\textbf{Remark: Used luminosity and number of events does not seem to be correct!}
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.7 \textwidth]{Chapters/Chapter6_Analysis/Figures/XSDistributions.pdf}
 \caption{Overview of the distribution of the generator-level cross-sections for different $\gR$ values and the reconstructed ones derived from them by applying the ratio $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$.} \label{fig::XSDistr}
\end{figure}

%--------------------------------------------------------------------------------------------------------- \\
%Also here there is an additional complexity when considering reconstructed events, since the cross-section of the $\ttbar$ decay depends on the value of the coupling coefficients in the interaction vertex. For generator-level events, these values are accesible for each generated sample since MadGraph automatically determines the cross-section of each generated process.
%\\
%Hence the cross-sections for these reconstructed events are derived from the MadGraph predictions by carefully calculating the generator-level cross-sections in a regime comparable to data. 
%This condition has been achieved by combining the cross-sections for each $\gR$ coefficients when no, one and two additional jets are included in the event. 
%This will not result in a perfect match to data, but will bring the considered configuration a bit closer to reality. 
%\\
%Since the cross-section should be include the effect of the event selection, the different MadGraph samples have to fullfill the different kinematic requirements given in Table~\ref{table::GenCuts}. The three different contributions are then added in order to obtain an overall cross-section for the \textbf{inclusive} 2-jet case, for which the results have been summarised in the second column of Table~\ref{table::XSValues}.
%The third column contains the cross-section values that will be applied for the measurement using the reconstructed events, and have been obtained by scaling the cross-section for each $\gR$ value with the fraction $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$. This ratio corrects the generator-level cross-sections to the expected reco-level one and can be applied onto all $\gR$ configurations since the relative effect of the event selection is already been incorporated by applying the basic event selection requirements on the MadGraph samples. The value $\sigma_{SM}^{reco}$ has been determined by dividing the number of selected events with the total number of events present in the sample and multiplying this with the cross-section of the semi-leptonic $\ttbar$ sample, which thus corresponds to multiplying the selected number of events with the luminosity of the simulated sample. The distribution of the generator-level cross-sections and the reconstructed ones is given in Figure~\ref{fig::XSDistr} and serves as an easy way to determine the reconstructed cross-section for other $\gR$ values if required.
%
%\begin{table}[h!t]
%  \centering
%  \caption{Overview of the cross-section values used for the reconstructed events together with the inclusive $\ttbar$+2jet events from which they have been derived. \textbf{Maybe redundant if figure is %given ..}} \label{table::XSValues}
%  \small
%  %\begin{tabular}{|c||c|c|c|c||c|}
%  \begin{tabular}{|c|c|c|}
%   \hline
%%   $\gR$ coefficient 	& $\ttbar$+0j ($\pbinv$) 	& $\ttbar$+1j ($\pbinv$) 	& $\ttbar$+2j ($\pbinv$) 	& Incl. $\ttbar$+2j 	& $\ttbar$ reco ($\pbinv$) 	\\
%   $\gR$ coefficient 	& Incl. $\ttbar$ + 2 jets ($\pbinv$) 	& $\ttbar$ reco ($\pbinv$) 	\\
%   \hline
%   \hline
%   -0.2  		& 3.34036				& 0.947244 			\\
%   -0.15		& 4.01717				& 1.13624			\\
%   -0.1 		& 4.84056				& 1.36448			\\
%   -0.05		& 5.82329 				& 1.63952			\\
%   0.0 		& 6.98981 				& 1.96892			\\
%   0.05		& 8.37733 				& 2.36027			\\
%   0.1 		& 10.0153 				& 2.82111			\\
%   0.15		& 11.9024 				& 3.35903			\\
%   0.2 		& 14.0873 				& 3.98157			\\
%   \hline
%  \end{tabular}
%\end{table}

\subsection{Matrix element event cleaning} \label{subsec::EvtCleaning}

With the correct reconstructed-level cross-sections determined, the actual measurement of the right-handed tensor coupling of the Wtb interaction vertex can be performed.
However, \textbf{at first sight} it appears that the Matrix Element method does not display similar behaviour as observed for the various generator-level studies since the outcome provided by the minimisation is not compatible with the expectations.
\\
A closer look at the issue indicated that the method is actually heavily influenced by badly calculated event topologies and the wider kinematic range allowed by the corresponding to the resolution functions developed for the reconstructed collision events.
This because the Matrix Element method will treat all events as if they were real semi-leptonic top-quark pair decays, and any inconsistency from the expected topology is likely to result in the underlying mathematical framework to misdetermine the event probability.
\\

The reason why these misdetermined events become particularly significant can be explained by the fact that the Matrix Element method assigns events for which the underlying mathematical framework cannot converge on average a lower probability. So even though the event probability is small compared to well-converging events, the conversion of the probability into a $\chiSqMEM$, or identically a $-\ln(\mathcal{L}_{MEM})$, implies that the former type of events will actually contribute the most to the overall $\chiSqMEM$ distribution.
\\

Before any action can be taken in order to reduce the contribution of the events not perfectly handled by the Matrix Element method, the origin of this \textit{behaviour} should be established.
Only after this is understood in detail, a dedicated event cleaning procedure can be developed in order to limit the influence of these type of events.
%
%Analysing reconstructed collision events using a Matrix Element method deviates significantly from any generator-level study, mainly because the former type of events tend to be influenced by detector inefficiencies, ill-defined kinematic variables or wrongly reconstructed event topologies. However, the Matrix Element method will treat all events as if they were real semi-leptonic top-quark pair decays, and any inconsistency from the expected topology is likely to result in the underlying mathematical framework to misdetermine the event probability.
%Therefore it is of crucial importance a procedure is developed in order to exclude the contribution of these sort of events, for which the details will be given in Section~\ref{subsec::EvtCleaning}.
%\\
%
%Even with the knowledge that the method behaving as should be and that the cross-section normalisation is applying the correct factor, the results obtained from the Matrix Element method need to surpass an additional cleaning procedure. This is a quality inherent to the inprecise determination of both the event kinematics and the event topology when considering reconstructed events. Any deviation from the expected topology might result in the the calculation technique not converging and thus misdetermining the event probability.
%
%Unfortunately these type of events can become rather significant since on average they appear to get a lower value assigned than well-converging events. Since the event probability is converted into a $\chi^{2}_{MEM}$ or $-\ln(\mathcal{L}_{MEM})$, events with a lower value actually contribute the strongest to the overall result. Hence, special care should be awarded to these type of misidentified events and they should be excluded in order not to bias the outcome.
%The influence of these type of events can be seen from Figure~\ref{fig::SMLik}, which contains the $\chiSqMEM$ of the $\gR$ = $0.0$ configuration for each event. This distributions shows a clear tail which does not exist for generator-level events, given in the right-handed figure.

\paragraph{Understanding the nature of the \textit{bad} events} \hfill \\

Simply applying the minimisation calculation of the Matrix Element method on the semi-leptonic $\ttbar$ sample containing all the reconstructed events for which the chosen jet combination correctly assigned each jet did not resulted in the expected (Standard Model) outcome. The curve describing the $\chiSqMEM$ distribution corresponded to a decreasing straight line such that the minimum was actually located at the edge of the considered range.
\\
However, since this simulated sample has been created using the Standard Model configuration, the minimisation calculation should retrieve a value located in the vicinity of the Standard Model expectation.
Especially because the detailed generator-level studies performed earlier indicated that both the model and method behave as expected.
%*********************
% Maybe still best to put the linearity test earlier since then it can be mentioned that the method can be trusted ...
%*********************
\\

In order to determine whether the events exhibiting this undesired behaviour have a specfic signature, \textit{a detailed investigation/an exhaustive study} has been performed.
\textit{\textbf{Comprehending}/Understanding/Grasping} the cause of this \textit{aberration/abnormality} inherent to the use of reconstructed collision events is a necessary first step towards the formation of a strategy to reduce \textit{\textbf{its}/this/their} contribution.
\\
Numerous variables have been looked at in order to find any indication for the disagreement between the generator- and reconstructed-level measurement, and the most promising one appeared to be the value of the $\chiSqMEM$-distribution evaluated at the Standard Model configuration ($\gR$ = 0.0).
\\
\\
Comparing the shape of this distribution for generator-level events and reconstructed collision events shows a distinctive difference, as can be seen from Figure~\ref{fig::SMLik}.
The dissimilarity (\textit{existing}) between the two is most prominent(\textit{ly visible}) in the tail, which is almost absent for the generated events but becomes extremely relevant for the selected ones.
Hence this seems to confirm that the Matrix Element method is heavily influenced by \textit{inefficiencies related to reconstructed events which do not occur for generator-level events.}
%***********************
% Try to be a bit more precise ...
%***********************
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLikelihoodValue_GenEventsSM.pdf} \hspace{0.3cm}
 %Taken from directory: Events_CalibCurve/CalibCurve_SemiMu_RgR_AllDeltaTF_MGSampleSM_20000Evts_CutsAlsoOnMET/SMLikelihoodValue_GenEventsSM.pdf
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLikelihoodValue_RecoEvents_MyTF.pdf}
 %--> Update this!!
 \caption{Distribution of the $\chi^{2}_{MEM}$-value for the $\gR$ = $0.0$ configuration for both the generator- (left) and reconstructed-level (right) events.} \label{fig::SMLik}
\end{figure}
%
%When comparing the generator-level distribution with the one for the reconstructed events can clearly be concluded that shape of the tail is the main distintion between the two.
%Hence, it has been studied whether by excluding the events residing in the tail a better minimum can be obtained.
%\\
%\\

The suggestion that this $\chiSqMEM$-variable is able to distinguish well reconstructed collision events from events influenced by inefficiencies is confirmed by the distributions shown in Figure~\ref{fig::SMLikCorrVSWr}.
\\
A confirmation of this interpretation is presented in Figure~\ref{fig::SMLikCorrVSWr}, which shows this $\chiSqMEM$-distribution for reconstructed $\ttbar$ events for which the topology has been correctly reconstructed and on the other hand events for which at least one jet has not been matched with the correct generator-level parton.
The distribution corresponding to the latter type of events is clearly shifted towards higher $\chiSqMEM$ values while the correctly reconstructed events peak at much lower values of $\chiSqMEM$.
\\
The second visible difference between the two sorts of reconstructed top-quark pair decays is the contrasting shape and amplitude of the tail.
This is significantly lower for the correctly reconstructed events, especially in the intermediate region of the $\chiSqMEM$ distribution which is thus more likely to be populated by badly reconstructed events.
\\
However, the slight increase of the tail around the region of $\chiSqMEM$ = 65 (\textbf{Will need to revise this value in case the $\chiSqMEM$ value is actually used in stead of currently the $\chiSqMEM/2$ one!!!}) seems to suggest that even for correctly reconstructed events a noteworthy number of events are located within the tail of the distribution. This reinforces the idea of the incorrect outcome being caused by inefficiencies by which the reconstructed events are more likely to be influenced even further. \textit{because it clearly seems to be related to the ...}
%**************************
% Think of some clear explanation
% --> Since it also appears for correctly reconstructed events it is shown to be inherent to the method and should thus be taken into account or corrected for ..!
%**************************
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/ScaledContribution_CWU_LumiNorm.pdf}
 \caption{\textit{Distribution for good (green) and wrong (red) reconstructed events ... Should still remove data and unmatched!}} \label{fig::SMLikCorrVSWr}
\end{figure}

%*************************
% Decide: Interesting to keep this??
%*************************
Finally, the importance of the applied resolution function can be visualised in Figure~\ref{fig::SMLikTF} which shows this $\chiSqMEM$ variable in case the resolution function developed in Section~\ref{sec::TF} is considered and otherwise in case the basic Gaussian function is used to describe the smearing of the \textbf{what exactly?}.
Comparing these two shows a clear dependence on this resolution function, again indicating that the events in the end of the tail cannot converge since the reconstructed kinematic information does not agree with the expectation within the range allowed by the resolution function.
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.35 \textwidth]{image.png} %Maybe show the two on top of each other? This way repeating the same figure can be avoided!!
 \caption{Distribution of the $\chi^{2}_{MEM}$-value for the $\gR$ = $0.0$ configuration for the resolution functions created specifically for this analysis (green) and for the basic Gaussian resolution function of the Matrix Element method (blue).} \label{Fig::SMLikTF}
\end{figure}

\paragraph{Developing the actual cleaning procedure} \hfill \\

Now that has been established how the discrepancy between generator-level events and reconstructed collision events can be identified, a procedure to get rid of the events causing it can be constructed.
Before deciding to just exclude all events above a specific $\chiSqMEM$ value, it has been examined whether some events with useful information actually reside in the tail of this distribution.
\\
Within the Matrix Element method events containing useful information are supposed to be characterised by a $\chiSqMEM$ curve \\
\textit{From the added 2D plots no real conclusion can be drawn unfortunately. The second derivative distribution was a possibility, but since it is just almost symmetric around zero it would only raise more questions when adding this distribution ...\\ So it seems that no actual explanation will be able to be given for this cut ...\\ Especially because the 2D plots which can be given indeed show that something goes wrong at the outer x-axis range but the behaviour at lower x-values is not as expected either ...}
%This has been done by investigating whether 
%This by probing into the specific characteristics of the individual events;
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/LnLikCut_ScatterPlot_SMLikvsMaxDelta_AllTT.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLik_vs_ScdDerFine_CorrectTTbar.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLik_vs_RightDeltaLnLik_CorrectTT.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLik_vs_LeftDeltaLnLik_CorrectTT.pdf}
 \caption{2D plot for all $\ttbar$ events (so CWU combined -- scdDer only for correct ...). The two lower plots contain on the y-axis the $\chiSqMEM$ difference between $\gR$ = 0.0 and $\gR$ = $\pm$ 0.2. Hence in order to have minimum at zero, both should actually be negative!} \label{fig::SMLik2D}
\end{figure}

Another important remark to make is that the applied cut on this $\chiSqMEM$ should not be set to tight since the contribution of badly reconstructed events is supposed to be rather low due to the stringent event selection criteria applied on the reconstructed events. Hence it is not the goal to reduce the events in the intermediate region of this $\chiSqMEM$ distribution, since their contribution is negligible compared to the total number of reconstructed events. This can be seen in Figure~\ref{fig::SMLikCorrVSWrUnSc}, which contains the same distributions as in Figure~\ref{fig::SMLikCorrVSWr} but now the histograms are not scaled and thus represent their actual contribution to the overall result. From this can be clearly seen that these simulated events are dominated by well-reconstructed event topologies.
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/RelativeContribution_CWU_LumiNorm.pdf}
 \caption{\textit{Remove the unmatched and data!}} \label{fig::SMLikCorrVSWrUnSc}
\end{figure}

\textit{Determine this cut-value using all background samples}\\
\textit{Show the fit-function used to get this optimal value!}
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/Data_vs_SeparateTTbarJetsSemiLept_NotLumiScaled.pdf}
 \caption{} \label{fig::OptCutValue}
\end{figure}

\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/LikCutFit_TTbarSemiLept.pdf}
 \caption{} \label{fig::CutValueFit}
\end{figure}


%--------------------------------------------------------------------------------------------------- \\
%Comparing the generator-level distribution with the one for the reconstructed events clearly shows that the events residing in the tail should be rejected. Considering different resolution functions has indicated that the shape and amplitude of the tail depends heavily on the applied resolution, indicating again that these type of events are less likely to be well reconstructed.
%Hence, the considered event-cleaning procedure will require the $\chi^{2}_{MEM}$ of each value to be lower than a specific cut-value. 
%\\
%
%The mentioned dependence on the resolution functions introduced for the reconstructed collision events can be visualised in Figure~\ref{fig::SMLikTF}, which contains this $\chiSqMEM$ variable at the $\gR$ = 0.0 point in case the resolution function developed in Section~\ref{sec::TF} is considered and in case a basic Gaussian function is used to describe the smearing of the \textbf{what exactly?}. 


\section{Results and systematics} \label{sec::Meas}

\subsection{Bias of reconstructed events}
The linearity test discussed in Section~\ref{subsec::CalibCurve} has indicated that the considered model does not introduce any biases and thus perfectly describes the decay of the top-quark. However, since this calibration has been determined using generator-level events, a final cross-check is required to ensure no bias is introduced when applying the full event-selection chain.
As mentioned before, the only simulation sample available describes the Standard Model configuration and can thus only be used to shift the calibration curve up or down depending on the obtained outcome.

\subsection{Pulls}

\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/PseudoExp_MinimumDistr_AllTTbarSemiLept.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/PseudoExp_UncDistr_AllTTbarSemiLept.pdf}
 \caption{Minimum and uncertainty distribution obtained for the 1000 considered pseudo experiments. (all semiLept TT)}  \caption{fig::MinAndUnc}
\end{figure}

\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/PullDistr_AllTTbarSemiLept.pdf}
 \caption{Pull distribution} \label{fig::PullDistr}
\end{figure}


