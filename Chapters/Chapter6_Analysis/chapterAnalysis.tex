\chapter{Measurement of anomalous couplings in top-quark decays} \label{ch::Analysis}

In this chapter the events passing the full event selection chain, outlined in Chapter~\ref{ch::EvtSel}, will be processed by the Matrix Element method, which has been discussed in detail in Chapter~\ref{ch::MW}. As has been mentioned in Chapter~\ref{chp::SM}, the measurement performed in this thesis will focus on the right-handed tensor coupling $\gR$ of the Wtb interaction vertex. All other coupling coefficients will be \textit{constraint to their Standard Model expectation value/will be kept as predicted by the Standard Model.}
%The full Lagrangian of the top-quark pair decay vertex is repeated in Equation (\ref{eq::FullWtbLagrMeas}), which simplifies to a single $V_L$-contribution in the absence of new-physics influences.
%\begin{eqnarray} \label{eq::FullWtbLagrMeas}
%  \mathcal{L}_{Wtb} & = & - \frac{g}{\sqrt{2}} \bar{b} \gamma^{\mu} \left( V_L P_L + V_R P_R \right) t W_{\mu}^{-} + h.c. \nonumber \\
%		    &   & - \frac{g}{\sqrt{2}} \bar{b} \frac{i\sigma^{\mu \nu} q_{\nu}}{m_{W}} \left( g_L P_L + g_R P_R \right) t W_{\mu}^{-} + h.c.
%\end{eqnarray}
%*********************************
% --> Good idea to repeat this equation here?
%*********************************
\\

However, in contrast to the top-mass measurement discussed in Section~\ref{sec::TopMass}, quite a few improvements to the method are required in order for it to be successfully applicable on reconstructed collision events.
\textit{The list of necessary adaptations will be introduced in Section~\ref{sec::RecoAdapt}, after which the framework of the Matrix Element method will be completely finalized to perform the actual measurement.
%Due to some inefficiencies specifically connected with the imperfections of reconstructed collision events, an additional event cleaning procedure will be outlined in Section~\ref{sec::Cleaning}
Then both the method and the model should be tested, which is done in Section~\ref{sec::CalibCurve}.
The determination of the coupling coefficient $\gR$ together with the systematic uncertainties by which the measurement is influenced, will be given in Section~\ref{sec::Meas}.}
%*******************************
% Check the final section division!!!
%*******************************

\section{Linearity test of the Matrix Element method}

\section{Required MEM framework adaptations} \label{sec::RecoAdapt}
%**********************
% Can also just make the section about the event cleaning but give this XS calculation as an introduction!
%  --> Work with paragraphs maybe ? (not so interesting to use paragrapsh in section ...)
%**********************

An important aspect of the Matrix Element method is the normalisation of the event probability using the cross-section and acceptance, which might vary significantly for the different configurations considered. For the top-quark mass example, the effect of this normalisation was negligible but for the measurement of the anomalous coupling coefficient this factor has proven to be rather important. 
Hence a dedicated calculation of these cross-section values has been done, as will be explained in Section~\ref{subsec::XSReco}.
%*********************************
% --> Check if this XS is actually important for the top-mass ..
%*********************************
\\

A second feature that requires some adaptations when considering reconstructed collision events is related with the possible detector inefficiencies, ill-defined kinematic variables and wrongly reconstructed event topologies by which these type of events might be affected.
%Analysing reconstructed collision events using a Matrix Element method deviates significantly from any generator-level study, mainly because the former type of events tend to be influenced by detector inefficiencies, ill-defined kinematic variables or wrongly reconstructed event topologies.
Since the Matrix Element method will treat all events as if they were real semi-leptonic top-quark pair decays, any inconsistency from the expected topology is likely to result in the underlying mathematical framework to misdetermine the event probability.
Therefore it is of crucial importance a procedure is developed in order to exclude the contribution of these sort of events, for which the details will be given in Section~\ref{subsec::EvtCleaning}.

%In order to correctly apply the Matrix Element method on reconstructed collision events, a different approach than the one used for generator-level events is required.
%At first, the ease \textbf{with which} generator-level samples with an alternative mass or coupling (\textit{Check whether any example of couplings is given ...}) could be created can not be repeated for these reconstructed events. Therefore, the linearity test performed to test the validity of the considered model assumptions will be performed using generator-level events, as explained in Section~\ref{subsec::CalibCurve}. Also the cross-section values at non-SM coupling coefficients for the reconstructed events need to be extrapolated from these generator-level events, a procedure which will be discussed in Section~\ref{subsec::XSReco}.
%\\
%Secondly, and more related to the actual application of the technique, the possible influence of detector inefficiencies, ill-defined kinematic variables or wrongly reconstructed event topologies has a detrimental effect on the calculation of the event probability. 
%Therefore an additional event cleaning procedure, outlined in detail in Section~\ref{subsec::EvtCleaning}, is required in order to limit the importance of these type of events. 

\subsection{Cross-section calculation} \label{subsec::XSReco}

The cross-section normalisation is a vital component for the measurement of the anomalous couplings, both on the generator as reconstructed level. Without this correction factor applied, the Matrix Element method minimisation procedure does not allow to retrieve the correct outcome. The considerable effect of this normalisation component has been summarised in Figure~\ref{fig::XSInflGen}, which shows the overall $\chiSqMEM$ distribution prior to and after the cross-section normalisation has been taken into account for a sample of 20 000 generator-level events produced with the Standard Model configuration.
%This because the observed variations of the overall event probability for the coupling coefficient are much smaller than was the case for the top-quark mass measurement such that the cross-section normalisation actually has a significant influence on the obtained outcome.
%***********************************
% --> Certain this is the reason?
% ==> Maybe good to think of an explanation why this cross-section normalisation is so much more important
%***********************************
\begin{figure}[h!t]
 \centering
 % Add here the gR gen-level result of FitDistributions_CalibCurve_SemiMu_RgR_AllDeltaTF_MGSampleSM_20000Evts_NoCuts_OuterBinsExclForFit_20000Evts.root with and without XS normalisation
 % Important: Cannot yet use the sample after the event-selection is applied because this still has to be explained!!
 % --> Do this without the fit maybe .. ?
 \includegraphics[width = 0.3 \textwidth]{image.png} \hspace{0.5cm}
 \includegraphics[width = 0.3 \textwidth]{image.png}
 \caption{Distribution of the overall $\chiSqMEM$-value obtained by analysing the right-handed tensor coupling using 20 000 generator-level events. The distribution on the left is without any normalisation applied while the right one corresponds to the normalised result.} \label{fig::XSInflGen}
\end{figure}

The substantial impact of the cross-section normalisation on the outcome of the measurement implies that the cross-section values for the reconstructed-level analysis should be determined with great care.
However, in contrast to the easy access to generator-level samples with alternative coupling coefficients, generating similar samples containing reconstructed events is a rather challenging and time-consuming process.
As a result, it has been opted for in this thesis to derive the cross-section values for the reconstructed events from the generator-level ones.
%\\
%However, it is important to note that in the remainder of this analysis the normalisation of the Matrix Element probability by the cross-section and the acceptance values will be combined into one single normalisation. This because the cross-section will be altered by the acceptance such that the latter can be taken into account in a rather straightforward manner.
%**********************
% Any other motivation why FastSim has not been considered?
% --> Certain that it would perfectly describe the SM samples??
%**********************
\\

The chosen approach of deriving the cross-sections from generator-level events significantly facilitates the cross-section determination since any generated process by MadGraph automatically calculates the cross-section of the considered process.
In order to ensure that the obtained generator-level cross-sections can easily be related to the reconstructed ones, the conditions present for the reconstructed collision events will be mimicked as closely as possible during the generation process. Hence the generator events have to fullfill a couple of rather basic event selection requirements\footnote{Important to note here is that once these selection criteria are applied to the generated events, the obtained cross-section will actually be a combination of the cross-section of the underlying physics process and the acceptance of the considered event selection. Hence the term ``cross-section normalisation'' will \textbf{implicitely} imply the combined normalisation $\sigma \times A$ mentioned in Equation~\ref{eq::MWEvtProb}.}, which have been listed in Table~\ref{table::GenCuts}.

\begin{table}[h!t]
 \centering
 \caption{Basic event selection applied to the generator-level events in order to partially mimic the situation existing for the reconstructed collision events.} \label{table::GenCuts}
 \begin{tabular}{c|c|c|c}
  Particle 	& $\pT$-cut 		& $\vert \eta \vert$-cut 	& $\Delta$R-cut 	\\
  \hline
  Jets 		& $>$ 30 $\GeV$ 	& $<$ 2.5			& $<$ 0.3		\\
  Muon		& $>$ 26 $\GeV$		& $<$ 2.1			& $<$ 0.3		\\
  Neutrino 	& $>$ 25 $\GeV$		& $<$ 2.5			& $<$ 0.3		
 \end{tabular}
\end{table}

By applying a significant fraction of the full event selection chain onto the generated events, the expected relative difference in behaviour of each $\gR$ value on the considered kinematic constraints will be incorporated. The remaining event selection criteria are believed to be less sensitive to the value of the coupling coefficient, thus their relative dependence will not be taken into account.
\\

The considered generator process was not merely the general semi-leptonic top-quark pair decay but, again motivated by the fact that the generator configuration should resemble as closely as possible the reconstructed one, a combination of $\ttbar$ decays surrounded by additonal jets.
The different decays for which generator-level samples have been created are the semi-leptonic $\ttbar$ decays with none, one and two additional jets present in the event.
%********************************************
% --> Does this correspond to LO, NLO and NNLO or is this still something different??
% Question: Interesting to give some of the Feynman diagrams belonging to the different processes?
%********************************************
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.15 \textwidth]{image.png} \hspace{0.2cm}
 \includegraphics[width = 0.15 \textwidth]{image.png} \hspace{0.2cm}
 \includegraphics[width = 0.15 \textwidth]{image.png}
 \caption{Feynman diagrams for the different generator-level processes considered for the calculation of the cross-sections.}
\end{figure}


This approach is not able to exactly describe the situation as it occurs for the selected events since, for example, it is not possible to include the effect of the applied b-tagging requirement \textit{in exactly the same way as in data}. Hence, the obtained cross-section values will need to be scaled in order to take into account the influence of the b-jet identification algorithm and other non-included event selection criteria such as the isolation, the invariant mass constraints, $\dots$ (\textbf{etc.}?).
For this scaling a flat behaviour throughout the entire $\gR$ range is assumed such that each cross-section value is multiplied with the factor $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$. 
%********************************************
% --> Think of any other non-included effects!!
%********************************************
\\
The cross-section for the Standard Model configuration of the selected events, denoted as $\sigma_{SM}^{reco}$, is determined by dividing the semi-leptonic $\ttbar$ event count obtained after the full event selection chain has been applied \textbf{with} the luminosity of this simulated sample, which can be found in Table~\ref{table::Samples}. The reconstructed cross-section values, together with the generator-level ones from which they have been derived, is given in Figure~\ref{fig::XSDistr}.
%Even though this will not result in a perfect match to the different decays present in the selected events, it will definitely ensure the considered configuration corresponds more to reality.
\\
\textbf{Remark: Used luminosity and number of events does not seem to be correct!}
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.7 \textwidth]{Chapters/Chapter6_Analysis/Figures/XSDistributions.pdf}
 \caption{Overview of the distribution of the generator-level cross-sections for different $\gR$ values and the reconstructed ones derived from them by applying the ratio $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$.} \label{fig::XSDistr}
\end{figure}

%--------------------------------------------------------------------------------------------------------- \\
%Also here there is an additional complexity when considering reconstructed events, since the cross-section of the $\ttbar$ decay depends on the value of the coupling coefficients in the interaction vertex. For generator-level events, these values are accesible for each generated sample since MadGraph automatically determines the cross-section of each generated process.
%\\
%Hence the cross-sections for these reconstructed events are derived from the MadGraph predictions by carefully calculating the generator-level cross-sections in a regime comparable to data. 
%This condition has been achieved by combining the cross-sections for each $\gR$ coefficients when no, one and two additional jets are included in the event. 
%This will not result in a perfect match to data, but will bring the considered configuration a bit closer to reality. 
%\\
%Since the cross-section should be include the effect of the event selection, the different MadGraph samples have to fullfill the different kinematic requirements given in Table~\ref{table::GenCuts}. The three different contributions are then added in order to obtain an overall cross-section for the \textbf{inclusive} 2-jet case, for which the results have been summarised in the second column of Table~\ref{table::XSValues}.
%The third column contains the cross-section values that will be applied for the measurement using the reconstructed events, and have been obtained by scaling the cross-section for each $\gR$ value with the fraction $\sigma_{SM}^{reco}$/$\sigma_{SM}^{gen}$. This ratio corrects the generator-level cross-sections to the expected reco-level one and can be applied onto all $\gR$ configurations since the relative effect of the event selection is already been incorporated by applying the basic event selection requirements on the MadGraph samples. The value $\sigma_{SM}^{reco}$ has been determined by dividing the number of selected events with the total number of events present in the sample and multiplying this with the cross-section of the semi-leptonic $\ttbar$ sample, which thus corresponds to multiplying the selected number of events with the luminosity of the simulated sample. The distribution of the generator-level cross-sections and the reconstructed ones is given in Figure~\ref{fig::XSDistr} and serves as an easy way to determine the reconstructed cross-section for other $\gR$ values if required.
%
%\begin{table}[h!t]
%  \centering
%  \caption{Overview of the cross-section values used for the reconstructed events together with the inclusive $\ttbar$+2jet events from which they have been derived. \textbf{Maybe redundant if figure is %given ..}} \label{table::XSValues}
%  \small
%  %\begin{tabular}{|c||c|c|c|c||c|}
%  \begin{tabular}{|c|c|c|}
%   \hline
%%   $\gR$ coefficient 	& $\ttbar$+0j ($\pbinv$) 	& $\ttbar$+1j ($\pbinv$) 	& $\ttbar$+2j ($\pbinv$) 	& Incl. $\ttbar$+2j 	& $\ttbar$ reco ($\pbinv$) 	\\
%   $\gR$ coefficient 	& Incl. $\ttbar$ + 2 jets ($\pbinv$) 	& $\ttbar$ reco ($\pbinv$) 	\\
%   \hline
%   \hline
%   -0.2  		& 3.34036				& 0.947244 			\\
%   -0.15		& 4.01717				& 1.13624			\\
%   -0.1 		& 4.84056				& 1.36448			\\
%   -0.05		& 5.82329 				& 1.63952			\\
%   0.0 		& 6.98981 				& 1.96892			\\
%   0.05		& 8.37733 				& 2.36027			\\
%   0.1 		& 10.0153 				& 2.82111			\\
%   0.15		& 11.9024 				& 3.35903			\\
%   0.2 		& 14.0873 				& 3.98157			\\
%   \hline
%  \end{tabular}
%\end{table}

\subsection{Matrix element event cleaning} \label{subsec::EvtCleaning}

With the correct reconstructed-level cross-sections determined, the actual measurement of the right-handed tensor coupling of the Wtb interaction vertex can be performed.
However, \textbf{at first sight} it appears that the Matrix Element method does not display similar behaviour as observed for the various generator-level studies since the outcome provided by the minimisation is not compatible with the expectations.
\\
A closer look at the issue indicated that the method is actually heavily influenced by badly calculated event topologies and the wider kinematic range allowed by the corresponding to the resolution functions developed for the reconstructed collision events.
This because the Matrix Element method will treat all events as if they were real semi-leptonic top-quark pair decays, and any inconsistency from the expected topology is likely to result in the underlying mathematical framework to misdetermine the event probability.
\\

The reason why these misdetermined events become particularly significant can be explained by the fact that the Matrix Element method assigns events for which the underlying mathematical framework cannot converge on average a lower probability. So even though the event probability is small compared to well-converging events, the conversion of the probability into a $\chiSqMEM$, or identically a $-\ln(\mathcal{L}_{MEM})$, implies that the former type of events will actually contribute the most to the overall $\chiSqMEM$ distribution.
\\

Before any action can be taken in order to reduce the contribution of the events not perfectly handled by the Matrix Element method, the origin of this \textit{behaviour} should be established.
Only after this is understood in detail, a dedicated event cleaning procedure can be developed in order to limit the influence of these type of events.
%
%Analysing reconstructed collision events using a Matrix Element method deviates significantly from any generator-level study, mainly because the former type of events tend to be influenced by detector inefficiencies, ill-defined kinematic variables or wrongly reconstructed event topologies. However, the Matrix Element method will treat all events as if they were real semi-leptonic top-quark pair decays, and any inconsistency from the expected topology is likely to result in the underlying mathematical framework to misdetermine the event probability.
%Therefore it is of crucial importance a procedure is developed in order to exclude the contribution of these sort of events, for which the details will be given in Section~\ref{subsec::EvtCleaning}.
%\\
%
%Even with the knowledge that the method behaving as should be and that the cross-section normalisation is applying the correct factor, the results obtained from the Matrix Element method need to surpass an additional cleaning procedure. This is a quality inherent to the inprecise determination of both the event kinematics and the event topology when considering reconstructed events. Any deviation from the expected topology might result in the the calculation technique not converging and thus misdetermining the event probability.
%
%Unfortunately these type of events can become rather significant since on average they appear to get a lower value assigned than well-converging events. Since the event probability is converted into a $\chi^{2}_{MEM}$ or $-\ln(\mathcal{L}_{MEM})$, events with a lower value actually contribute the strongest to the overall result. Hence, special care should be awarded to these type of misidentified events and they should be excluded in order not to bias the outcome.
%The influence of these type of events can be seen from Figure~\ref{fig::SMLik}, which contains the $\chiSqMEM$ of the $\gR$ = $0.0$ configuration for each event. This distributions shows a clear tail which does not exist for generator-level events, given in the right-handed figure.

\paragraph{Understanding the nature of the \textit{bad} events} \hfill \\

Simply applying the minimisation calculation of the Matrix Element method on the semi-leptonic $\ttbar$ sample containing all the reconstructed events for which the chosen jet combination correctly assigned each jet did not resulted in the expected (Standard Model) outcome. The curve describing the $\chiSqMEM$ distribution corresponded to a decreasing straight line such that the minimum was actually located at the edge of the considered range.
\\
However, since this simulated sample has been created using the Standard Model configuration, the minimisation calculation should retrieve a value located in the vicinity of the Standard Model expectation.
Especially because the detailed generator-level studies performed earlier indicated that both the model and method behave as expected.
%*********************
% Maybe still best to put the linearity test earlier since then it can be mentioned that the method can be trusted ...
%*********************
\\

In order to determine whether the events exhibiting this undesired behaviour have a specfic signature, \textit{a detailed investigation/an exhaustive study} has been performed.
\textit{\textbf{Comprehending}/Understanding/Grasping} the cause of this \textit{aberration/abnormality} inherent to the use of reconstructed collision events is a necessary first step towards the formation of a strategy to reduce \textit{\textbf{its}/this/their} contribution.
\\
Numerous variables have been looked at in order to find any indication for the disagreement between the generator- and reconstructed-level measurement, and the most promising one appeared to be the value of the $\chiSqMEM$-distribution evaluated at the Standard Model configuration ($\gR$ = 0.0).
\\
\\
Comparing the shape of this distribution for generator-level events and reconstructed collision events shows a distinctive difference, as can be seen from Figure~\ref{fig::SMLik}.
The dissimilarity (\textit{existing}) between the two is most prominent(\textit{ly visible}) in the tail, which is almost absent for the generated events but becomes extremely relevant for the selected ones.
Hence this seems to confirm that the Matrix Element method is heavily influenced by \textit{inefficiencies related to reconstructed events which do not occur for generator-level events.}
%***********************
% Try to be a bit more precise ...
%***********************
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLikelihoodValue_GenEventsSM.pdf} \hspace{0.3cm}
 %Taken from directory: Events_CalibCurve/CalibCurve_SemiMu_RgR_AllDeltaTF_MGSampleSM_20000Evts_CutsAlsoOnMET/SMLikelihoodValue_GenEventsSM.pdf
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLikelihoodValue_RecoEvents_MyTF.pdf}
 %--> Update this!!
 \caption{Distribution of the $\chi^{2}_{MEM}$-value for the $\gR$ = $0.0$ configuration for both the generator- (left) and reconstructed-level (right) events.} \label{fig::SMLik}
\end{figure}
%
%When comparing the generator-level distribution with the one for the reconstructed events can clearly be concluded that shape of the tail is the main distintion between the two.
%Hence, it has been studied whether by excluding the events residing in the tail a better minimum can be obtained.
%\\
%\\

The suggestion that this $\chiSqMEM$-variable is able to distinguish well reconstructed collision events from events influenced by inefficiencies is confirmed by the distributions shown in Figure~\ref{fig::SMLikCorrVSWr}.
\\
A confirmation of this interpretation is presented in Figure~\ref{fig::SMLikCorrVSWr}, which shows this $\chiSqMEM$-distribution for reconstructed $\ttbar$ events for which the topology has been correctly reconstructed and on the other hand events for which at least one jet has not been matched with the correct generator-level parton.
The distribution corresponding to the latter type of events is clearly shifted towards higher $\chiSqMEM$ values while the correctly reconstructed events peak at much lower values of $\chiSqMEM$.
\\
The second visible difference between the two sorts of reconstructed top-quark pair decays is the contrasting shape and amplitude of the tail.
This is significantly lower for the correctly reconstructed events, especially in the intermediate region of the $\chiSqMEM$ distribution which is thus more likely to be populated by badly reconstructed events.
\\
However, the slight increase of the tail around the region of $\chiSqMEM$ = 65 (\textbf{Will need to revise this value in case the $\chiSqMEM$ value is actually used in stead of currently the $\chiSqMEM/2$ one!!!}) seems to suggest that even for correctly reconstructed events a noteworthy number of events are located within the tail of the distribution. This reinforces the idea of the incorrect outcome being caused by inefficiencies by which the reconstructed events are more likely to be influenced even further. \textit{because it clearly seems to be related to the ...}
%**************************
% Think of some clear explanation
% --> Since it also appears for correctly reconstructed events it is shown to be inherent to the method and should thus be taken into account or corrected for ..!
%**************************
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/ScaledContribution_CWU_LumiNorm.pdf}
 \caption{\textit{Distribution for good (green) and wrong (red) reconstructed events ... Should still remove data and unmatched!}} \label{fig::SMLikCorrVSWr}
\end{figure}

%*************************
% Decide: Interesting to keep this??
%*************************
Finally, the importance of the applied resolution function can be visualised in Figure~\ref{fig::SMLikTF} which shows this $\chiSqMEM$ variable in case the resolution function developed in Section~\ref{sec::TF} is considered and otherwise in case the basic Gaussian function is used to describe the smearing of the \textbf{what exactly?}.
Comparing these two shows a clear dependence on this resolution function, again indicating that the events in the end of the tail cannot converge since the reconstructed kinematic information does not agree with the expectation within the range allowed by the resolution function.
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.35 \textwidth]{image.png} %Maybe show the two on top of each other? This way repeating the same figure can be avoided!!
 \caption{Distribution of the $\chi^{2}_{MEM}$-value for the $\gR$ = $0.0$ configuration for the resolution functions created specifically for this analysis (green) and for the basic Gaussian resolution function of the Matrix Element method (blue).} \label{Fig::SMLikTF}
\end{figure}

\paragraph{Developing the actual cleaning procedure} \hfill \\

Now that has been established how the discrepancy between generator-level events and reconstructed collision events can be identified, a procedure to get rid of the events causing it can be constructed.
Before deciding to just exclude all events above a specific $\chiSqMEM$ value, it has been examined whether some events with useful information actually reside in the tail of this distribution.
\\
Within the Matrix Element method events containing useful information are supposed to be characterised by a $\chiSqMEM$ curve \\
\textit{From the added 2D plots no real conclusion can be drawn unfortunately. The second derivative distribution was a possibility, but since it is just almost symmetric around zero it would only raise more questions when adding this distribution ...\\ So it seems that no actual explanation will be able to be given for this cut ...\\ Especially because the 2D plots which can be given indeed show that something goes wrong at the outer x-axis range but the behaviour at lower x-values is not as expected either ...}
%This has been done by investigating whether 
%This by probing into the specific characteristics of the individual events;
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/LnLikCut_ScatterPlot_SMLikvsMaxDelta_AllTT.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLik_vs_ScdDerFine_CorrectTTbar.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLik_vs_RightDeltaLnLik_CorrectTT.pdf}
 \includegraphics[width = 0.45 \textwidth]{Chapters/Chapter6_Analysis/Figures/SMLik_vs_LeftDeltaLnLik_CorrectTT.pdf}
 \caption{2D plot for all $\ttbar$ events (so CWU combined -- scdDer only for correct ...). The two lower plots contain on the y-axis the $\chiSqMEM$ difference between $\gR$ = 0.0 and $\gR$ = $\pm$ 0.2. Hence in order to have minimum at zero, both should actually be negative!} \label{fig::SMLik2D}
\end{figure}

Another important remark to make is that the applied cut on this $\chiSqMEM$ should not be set to tight since the contribution of badly reconstructed events is supposed to be rather low due to the stringent event selection criteria applied on the reconstructed events. Hence it is not the goal to reduce the events in the intermediate region of this $\chiSqMEM$ distribution, since their contribution is negligible compared to the total number of reconstructed events. This can be seen in Figure~\ref{fig::SMLikCorrVSWrUnSc}, which contains the same distributions as in Figure~\ref{fig::SMLikCorrVSWr} but now the histograms are not scaled and thus represent their actual contribution to the overall result. From this can be clearly seen that these simulated events are dominated by well-reconstructed event topologies.
\\
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/RelativeContribution_CWU_LumiNorm.pdf}
 \caption{\textit{Remove the unmatched and data!}} \label{fig::SMLikCorrVSWrUnSc}
\end{figure}


\textit{Determine this cut-value using all background samples}\\
\textit{Show the fit-function used to get this optimal value!}
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.65 \textwidth]{Chapters/Chapter6_Analysis/Figures/Data_vs_SeparateTTbarJetsSemiLept_NotLumiScaled.pdf}
 \caption{} \label{fig::OptCutValue}
\end{figure}


--------------------------------------------------------------------------------------------------- \\
Comparing the generator-level distribution with the one for the reconstructed events clearly shows that the events residing in the tail should be rejected. Considering different resolution functions has indicated that the shape and amplitude of the tail depends heavily on the applied resolution, indicating again that these type of events are less likely to be well reconstructed.
Hence, the considered event-cleaning procedure will require the $\chi^{2}_{MEM}$ of each value to be lower than a specific cut-value. 
\\

The mentioned dependence on the resolution functions introduced for the reconstructed collision events can be visualised in Figure~\ref{fig::SMLikTF}, which contains this $\chiSqMEM$ variable at the $\gR$ = 0.0 point in case the resolution function developed in Section~\ref{sec::TF} is considered and in case a basic Gaussian function is used to describe the smearing of the \textbf{what exactly?}. 


\section{Linearity test of Matrix Element method}\label{sec::CalibCurve}

The absence of simulated samples with alternative coupling coefficients necessitates the created model to be tested using generator-level results. This linearity test verifies whether the different model approximations did not enter a bias for the obtained result by measuring the parameter of interest in a wide range around the expectation.
\\
As was mentioned in Section~\ref{sec::SubWtb}, this analysis will focus on the right-handed tensor coupling of the Wtb interaction, $g_{R}$, which has an expectation value of $0$ in the Standard Model. Therefore the performed linearity test will only considered the range $\gR$ $\in$ $\left[-0.15, 0.15\right]$, which will be sufficient to obtain a precise determination of any possible bias present in either the model or method.
\\

Since the event selection is a rather likely source for introducing a bias, due to the possible different kinematic behaviour caused by the alternative coupling coefficient in the interaction Lagrangian, the linearity test will be performed using generator-level events with a basic event selection applied.
This does not perfectly match with the full event selection that will be applied for the reconstructed events, but by applying the major kinematic constraints a large fraction of the cuts can be incorporated. Also because the effects of applying the fine-tuning criteria, discussed in Section~\ref{sec::SpecificSelec}, are expected to be less influenced by the value of the coupling coefficient. The different cuts applied to the generator-level events have been summarised in Table~\ref{table::GenCuts}.


The different generator-level samples considered here all contain 20 000 events, a value chosen to be close to the number of selected data events, and have been created using the MadGraph event-generation process. A total of 13 samples have been created, each with a different coupling coefficient ranging between $-0.4$ and $0.4$, to ensure the entire range of interest was covered. 
However, afterwards it has been decided not no considered the samples lying outside of the above-mentioned range of $\left[-0.15, 0.15\right]$. This because the obtained results indicated that the considered measurement is precise enough to ensure the observed bias does not become larger than $0.15$.
%Question: Mention that this decision was also driven by the fact that the fit is not accurate enough because there is not enough bin-information outside this range??
\\

All these samples have then been analysed by the Matrix Element method and the corresponding minimum of each sample determined, which can then be compared with the expected value of this sample.
Since for a consistent model and method these two values have to agree, their distribution should be described by a straight line with a slope equal to $1$. Such a shape has indeed been retrieved and is shown in Figure~\ref{fig::CalibCurve}. 
%This minimum is then compared with the coefficient value of the sample and  in order to ensure consistency of the both the method and model. This is clearly achieved, as can be seen from Figure~\ref{fig::CalibCurve}, 
%Question: Fit also done by excluding the outer bins?
\begin{figure}[h!t]
 \centering
 \includegraphics[width = 0.5 \textwidth]{image.png}
 \caption{Outcome of the linearity test for the created model, indicating the absence of a bias due to the assumed model approximations. The calibration is therefore described perfectly since it can be represented by a straight line with slope $1$.} \label{fig::CalibCurve}
\end{figure}

Since the performed linearity proves that the introduced model does not result in a bias and thus the measurements obtained with the discussed model can be trusted, no calibration will be applied to the obtained outcome. The small deviation from the expected curve is perfectly covered by the corresponding uncertainty (\textbf{Check if this is also the case for the intersect value}) and should thus not be taken into account.

%This means that the linearity test to check whether the model approximations might have introduced a bias has to be performed using the generator-level results and afterwards the entire curve needs to be shifted to the correspond with the result obtained from the signal $\ttbar$ sample.

\section{Results and systematics} \label{sec::Meas}

\subsection{Bias of reconstructed events}
The linearity test discussed in Section~\ref{subsec::CalibCurve} has indicated that the considered model does not introduce any biases and thus perfectly describes the decay of the top-quark. However, since this calibration has been determined using generator-level events, a final cross-check is required to ensure no bias is introduced when applying the full event-selection chain.
As mentioned before, the only simulation sample available describes the Standard Model configuration and can thus only be used to shift the calibration curve up or down depending on the obtained outcome.
\\
